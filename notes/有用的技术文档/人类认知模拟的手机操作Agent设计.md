# äººç±»è®¤çŸ¥æ¨¡æ‹Ÿçš„æ‰‹æœºæ“ä½œ Agent è®¾è®¡

## ä¸€ã€æ ¸å¿ƒç†å¿µï¼šåƒäººä¸€æ ·æ€è€ƒå’Œæ“ä½œ

> **è®¾è®¡å“²å­¦**ï¼šä¸æ˜¯è®© AI æ‰§è¡Œé¢„å®šä¹‰çš„æ“ä½œåºåˆ—ï¼Œè€Œæ˜¯è®© AI **åƒäººç±»ä¸€æ ·è§‚å¯Ÿã€æ€è€ƒã€å°è¯•ã€çº é”™**

### äººç±»æ“ä½œæ‰‹æœºçš„è®¤çŸ¥è¿‡ç¨‹

```
ä»»åŠ¡å¼€å§‹
    â†“
[è§‚å¯Ÿ] æ‰«è§†å±å¹•ï¼Œè¯†åˆ«å…³é”®å…ƒç´ 
    â†“
[æ€è€ƒ] åŸºäºå½“å‰ç”»é¢ï¼Œåˆ¤æ–­ä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆ
    â†“
[å†³ç­–] é€‰æ‹©ä¸€ä¸ªæ“ä½œï¼ˆç‚¹å‡»/æ»‘åŠ¨/è¾“å…¥ï¼‰
    â†“
[æ‰§è¡Œ] æ‰§è¡Œæ“ä½œ
    â†“
[å³æ—¶åé¦ˆ] è§‚å¯Ÿå±å¹•å˜åŒ–
    â†“
[è¯„ä¼°] è¿™ä¸€æ­¥å¯¹å—ï¼Ÿæ˜¯å¦æœç€ç›®æ ‡å‰è¿›ï¼Ÿ
    â”œâ”€ æ­£ç¡® â†’ ç»§ç»­ä¸‹ä¸€æ­¥
    â”œâ”€ ä¸ç¡®å®š â†’ å†è§‚å¯Ÿï¼Œæˆ–å°è¯•å…¶ä»–è·¯å¾„
    â””â”€ é”™è¯¯ â†’ ç«‹å³å›é€€ï¼ˆæŒ‰è¿”å›é”®ï¼‰
```

**å…³é”®ç‰¹å¾**ï¼š
1. **æŒç»­è§‚å¯Ÿ** - äººä¸ä¼š"ç›²æ“ä½œ"ï¼Œæ¯ä¸€æ­¥éƒ½çœ‹ç€å±å¹•
2. **çŸ­æœŸè®°å¿†** - è®°ä½åˆšæ‰åšäº†ä»€ä¹ˆï¼Œé¿å…åŸåœ°æ‰“è½¬
3. **è¯•é”™å­¦ä¹ ** - ç‚¹é”™äº†ç«‹åˆ»æ„è¯†åˆ°ï¼Œä¸ä¼šé‡å¤é”™è¯¯
4. **åŠ¨æ€è§„åˆ’** - æ²¡æœ‰å›ºå®šè„šæœ¬ï¼Œæ ¹æ®å®æ—¶ç”»é¢å†³ç­–
5. **æ¨¡ç³Šæ¨ç†** - å³ä½¿ UI å˜åŒ–ï¼Œä¹Ÿèƒ½æ‰¾åˆ°"å¤§æ¦‚åœ¨é‚£ä¸ªä½ç½®"çš„æŒ‰é’®

---

## äºŒã€äººç±»è®¤çŸ¥æ¨¡å‹æ‹†è§£

### 2.1 è§†è§‰æ„ŸçŸ¥ç³»ç»Ÿï¼ˆVision Systemï¼‰

#### äººç±»çš„è§†è§‰å¤„ç†è¿‡ç¨‹

```
å±å¹•ç”»é¢
    â†“
[é¢„æ³¨æ„é˜¶æ®µ] - å¿«é€Ÿæ‰«æï¼Œ0.1-0.3 ç§’
  â”œâ”€ æ£€æµ‹æ˜¾è‘—ç‰©ä½“ï¼ˆå›¾æ ‡ã€æŒ‰é’®ï¼‰
  â”œâ”€ è¯†åˆ«é¢œè‰²/å½¢çŠ¶æ¨¡å¼
  â””â”€ è¿‡æ»¤æ— å…³ä¿¡æ¯ï¼ˆèƒŒæ™¯ã€è£…é¥°ï¼‰
    â†“
[æ³¨æ„åŠ›èšç„¦] - æ·±åº¦è¯†åˆ«ï¼Œ0.5-1 ç§’
  â”œâ”€ é˜…è¯»æ–‡å­—å†…å®¹
  â”œâ”€ ç†è§£ç©ºé—´å…³ç³»ï¼ˆä¸Šä¸‹å·¦å³ï¼‰
  â””â”€ è¯†åˆ«å¯äº¤äº’å…ƒç´ 
    â†“
[è¯­ä¹‰ç†è§£] - æ•´åˆä¸Šä¸‹æ–‡ï¼Œ1-2 ç§’
  â”œâ”€ ç†è§£å½“å‰é¡µé¢æ˜¯ä»€ä¹ˆï¼ˆé¦–é¡µ/æœç´¢é¡µ/è¯¦æƒ…é¡µï¼‰
  â”œâ”€ æ¨ç†å¯èƒ½çš„æ“ä½œè·¯å¾„
  â””â”€ ä¸ä»»åŠ¡ç›®æ ‡å»ºç«‹å…³è”
```

#### AI æ¨¡æ‹Ÿå®ç°

```python
class HumanLikeVisionSystem:
    """æ¨¡æ‹Ÿäººç±»è§†è§‰æ„ŸçŸ¥ç³»ç»Ÿ"""

    def __init__(self):
        self.vl_model = VLModel("Qwen2-VL-72B")  # æœ¬åœ°å¼ºåŠ›å¤šæ¨¡æ€æ¨¡å‹
        self.ocr_model = PaddleOCR()
        self.ui_detector = UIDetector()  # æ£€æµ‹å¯äº¤äº’å…ƒç´ 

    async def perceive_screen(self, screenshot: Image) -> PerceptionResult:
        """
        æ¨¡æ‹Ÿäººç±»çš„å¤šå±‚æ¬¡è§†è§‰æ„ŸçŸ¥
        ä¸è€ƒè™‘ token æ¶ˆè€—ï¼Œå…¨é¢åˆ†æ
        """

        # ===== å±‚æ¬¡ 1: å¿«é€Ÿæ‰«æï¼ˆé¢„æ³¨æ„ï¼‰ =====
        quick_scan = await self._quick_scan(screenshot)
        # æå–ï¼šé¢œè‰²åˆ†å¸ƒã€æ˜¾è‘—åŒºåŸŸã€å¸ƒå±€ç»“æ„

        # ===== å±‚æ¬¡ 2: æ·±åº¦è¯†åˆ«ï¼ˆæ³¨æ„åŠ›èšç„¦ï¼‰ =====
        # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡ï¼ˆä¸åœ¨ä¹ tokenï¼‰
        ocr_result, ui_elements, visual_features = await asyncio.gather(
            self._extract_all_text(screenshot),      # OCR å…¨æ–‡æœ¬
            self._detect_ui_elements(screenshot),    # UI å…ƒç´ æ£€æµ‹
            self._extract_visual_features(screenshot) # è§†è§‰ç‰¹å¾
        )

        # ===== å±‚æ¬¡ 3: è¯­ä¹‰ç†è§£ï¼ˆæ•´åˆï¼‰ =====
        # ä½¿ç”¨å¤§æ¨¡å‹æ·±åº¦ç†è§£å½“å‰ç”»é¢
        semantic_understanding = await self._deep_understanding(
            screenshot=screenshot,
            ocr_result=ocr_result,
            ui_elements=ui_elements,
            quick_scan=quick_scan
        )

        return PerceptionResult(
            screen_type=semantic_understanding.screen_type,  # é¦–é¡µ/åˆ—è¡¨é¡µ/è¯¦æƒ…é¡µ
            key_elements=semantic_understanding.key_elements,  # å…³é”®å¯ç‚¹å‡»å…ƒç´ 
            text_content=ocr_result,
            spatial_layout=ui_elements,
            attention_focus=self._compute_attention_map(screenshot),  # çƒ­åŠ›å›¾
            actionable_items=self._extract_actionable_items(ui_elements),
            current_context=semantic_understanding.context  # å½“å‰ä¸Šä¸‹æ–‡ç†è§£
        )

    async def _deep_understanding(self, screenshot, ocr_result, ui_elements, quick_scan):
        """
        æ·±åº¦è¯­ä¹‰ç†è§£ - ä¸é™åˆ¶ tokenï¼Œå…¨é¢åˆ†æ
        """

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ­£åœ¨çœ‹æ‰‹æœºå±å¹•çš„äººç±»ã€‚è¯·ä»”ç»†è§‚å¯Ÿè¿™ä¸ªå±å¹•å¹¶å›ç­”ï¼š

# å½“å‰å±å¹•åˆ†æ

## 1. è¿™æ˜¯ä»€ä¹ˆé¡µé¢ï¼Ÿ
- é¡µé¢ç±»å‹ï¼ˆé¦–é¡µ/æœç´¢é¡µ/åˆ—è¡¨é¡µ/è¯¦æƒ…é¡µ/è®¾ç½®é¡µ/...ï¼‰
- å±äºå“ªä¸ª App
- é¡µé¢çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆ

## 2. å±å¹•ä¸Šæœ‰å“ªäº›é‡è¦å…ƒç´ ï¼Ÿ
åˆ—å‡ºæ‰€æœ‰å¯äº¤äº’çš„å…ƒç´ ï¼ŒåŒ…æ‹¬ï¼š
- æŒ‰é’®ï¼ˆä½ç½®ã€æ–‡å­—ã€åŠŸèƒ½ï¼‰
- è¾“å…¥æ¡†ï¼ˆä½ç½®ã€æç¤ºæ–‡å­—ã€ç”¨é€”ï¼‰
- åˆ—è¡¨é¡¹ï¼ˆå¦‚æœæœ‰ï¼‰
- å¯¼èˆªæ /æ ‡ç­¾æ 

## 3. å½“å‰å¯ä»¥åšä»€ä¹ˆæ“ä½œï¼Ÿ
åŸºäºè¿™ä¸ªé¡µé¢ï¼Œåˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„æ“ä½œåŠå…¶æ•ˆæœ

## 4. å¦‚æœæˆ‘çš„ç›®æ ‡æ˜¯ [ä»å·¥ä½œè®°å¿†è·å–ç›®æ ‡]ï¼Œä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆï¼Ÿ
ç»™å‡ºæ¨ç†è¿‡ç¨‹å’Œå»ºè®®æ“ä½œ

## è¾…åŠ©ä¿¡æ¯
- OCR è¯†åˆ«çš„æ–‡å­—ï¼š{ocr_result}
- æ£€æµ‹åˆ°çš„ UI å…ƒç´ ï¼š{ui_elements}
- å¸ƒå±€æ¦‚è§ˆï¼š{quick_scan}
"""

        # å¤šè½®å¯¹è¯å¼åˆ†æï¼ˆæ¨¡æ‹Ÿäººç±»çš„åå¤è§‚å¯Ÿï¼‰
        response = await self.vl_model.analyze(
            image=screenshot,
            prompt=prompt,
            max_tokens=4000,  # ä¸é™åˆ¶ï¼Œè®©æ¨¡å‹å……åˆ†è¡¨è¾¾
            temperature=0.1
        )

        # å¦‚æœç¬¬ä¸€æ¬¡åˆ†æä¸å¤Ÿæ¸…æ™°ï¼Œè¿½é—®
        if response.confidence < 0.7:
            followup = await self.vl_model.analyze(
                image=screenshot,
                prompt=f"åˆšæ‰çš„åˆ†ææ˜¯ï¼š{response.text}\n\nè¯·å†ä»”ç»†çœ‹ä¸€éï¼Œè¡¥å……é—æ¼çš„ä¿¡æ¯ã€‚",
                max_tokens=2000
            )
            response = self._merge_analysis(response, followup)

        return response

    def _compute_attention_map(self, screenshot: Image) -> AttentionMap:
        """
        è®¡ç®—æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼ˆæ¨¡æ‹Ÿäººçœ¼æ‰«è§†è·¯å¾„ï¼‰

        åŸºäºè§†è§‰æ˜¾è‘—æ€§ç®—æ³•ï¼š
        1. é¢œè‰²å¯¹æ¯”åº¦
        2. è¾¹ç¼˜å¯†åº¦
        3. æ–‡å­—åŒºåŸŸ
        4. å·²çŸ¥çš„ UI æ¨¡å¼ï¼ˆæŒ‰é’®é€šå¸¸åœ¨åº•éƒ¨/å³ä¸Šè§’ï¼‰
        """

        # ä½¿ç”¨æ˜¾è‘—æ€§æ£€æµ‹ç®—æ³•
        saliency_map = cv2.saliency.StaticSaliencySpectralResidual_create()
        (success, saliency) = saliency_map.computeSaliency(np.array(screenshot))

        # å¢å¼ºæ–‡å­—åŒºåŸŸçš„æƒé‡ï¼ˆäººæ›´å…³æ³¨æ–‡å­—ï¼‰
        text_regions = self._detect_text_regions(screenshot)
        for region in text_regions:
            saliency[region.y:region.y+region.h, region.x:region.x+region.w] *= 1.5

        # å¢å¼ºå¯äº¤äº’å…ƒç´ çš„æƒé‡
        ui_elements = self._detect_ui_elements(screenshot)
        for elem in ui_elements:
            saliency[elem.bbox] *= 1.3

        return AttentionMap(heatmap=saliency)
```

---

### 2.2 å·¥ä½œè®°å¿†ç³»ç»Ÿï¼ˆWorking Memoryï¼‰

#### äººç±»å·¥ä½œè®°å¿†ç‰¹ç‚¹

```
å·¥ä½œè®°å¿†å®¹é‡ï¼š7Â±2 ä¸ªä¿¡æ¯å—ï¼ˆç±³å‹’å®šå¾‹ï¼‰
æŒç»­æ—¶é—´ï¼š20-30 ç§’ï¼ˆä¸åˆ·æ–°ä¼šé—å¿˜ï¼‰
å†…å®¹ï¼š
  â”œâ”€ å½“å‰ç›®æ ‡ï¼š"æˆ‘è¦ä¹°åŒè‚©åŒ…"
  â”œâ”€ æœ€è¿‘ 3-5 æ­¥æ“ä½œï¼š"æ‰“å¼€æ·˜å® â†’ ç‚¹å‡»æœç´¢ â†’ è¾“å…¥å…³é”®è¯"
  â”œâ”€ å½“å‰ä¸Šä¸‹æ–‡ï¼š"ç°åœ¨åœ¨æœç´¢ç»“æœé¡µ"
  â””â”€ ä¸´æ—¶å˜é‡ï¼š"åˆšæ‰çœ‹åˆ°ç¬¬ 3 ä¸ªå•†å“ä¸é”™"
```

#### AI å®ç°ï¼šåŠ¨æ€å·¥ä½œè®°å¿†

```python
from collections import deque
from datetime import datetime, timedelta

class WorkingMemory:
    """
    æ¨¡æ‹Ÿäººç±»å·¥ä½œè®°å¿†
    ç‰¹ç‚¹ï¼š
    1. æœ‰é™å®¹é‡ï¼ˆæœ€è¿‘ N æ­¥ï¼‰
    2. æ—¶é—´è¡°å‡ï¼ˆæ—§çš„ä¼šé—å¿˜ï¼‰
    3. é‡è¦æ€§åŠ æƒï¼ˆå…³é”®æ­¥éª¤è®°å¾—æ›´ä¹…ï¼‰
    """

    def __init__(self, capacity=7):
        self.capacity = capacity
        self.memory_buffer = deque(maxlen=capacity)
        self.current_goal = None
        self.sub_goals = []  # å­ç›®æ ‡æ ˆ
        self.context_snapshot = {}  # å½“å‰ä¸Šä¸‹æ–‡å¿«ç…§

    def set_goal(self, goal: str):
        """è®¾ç½®ä¸»ç›®æ ‡ï¼ˆæœ€é‡è¦ï¼Œä¸ä¼šè¢«é—å¿˜ï¼‰"""
        self.current_goal = goal

    def push_sub_goal(self, sub_goal: str):
        """
        å‹å…¥å­ç›®æ ‡
        ç¤ºä¾‹ï¼š
        ä¸»ç›®æ ‡ï¼šä¹°åŒè‚©åŒ…
        å­ç›®æ ‡æ ˆï¼š[æ‰“å¼€æ·˜å®, æœç´¢å•†å“, é€‰æ‹©å•†å“] â† å½“å‰åœ¨è¿™
        """
        self.sub_goals.append(sub_goal)

    def pop_sub_goal(self):
        """å®Œæˆä¸€ä¸ªå­ç›®æ ‡"""
        if self.sub_goals:
            completed = self.sub_goals.pop()
            self.add_memory(f"âœ“ å®Œæˆå­ç›®æ ‡: {completed}", importance=0.8)

    def add_memory(self, content: str, importance: float = 0.5):
        """
        æ·»åŠ è®°å¿†æ¡ç›®
        importance: 0-1ï¼Œå†³å®šé—å¿˜é€Ÿåº¦
        """
        entry = MemoryEntry(
            content=content,
            timestamp=datetime.now(),
            importance=importance,
            access_count=0
        )
        self.memory_buffer.append(entry)

    def recall(self, query: str = None) -> List[MemoryEntry]:
        """
        å›å¿†ï¼ˆæ£€ç´¢å·¥ä½œè®°å¿†ï¼‰
        æ¨¡æ‹Ÿäººç±»çš„"æƒ³ä¸€æƒ³åˆšæ‰åšäº†ä»€ä¹ˆ"
        """

        # æ—¶é—´è¡°å‡ï¼šè¶…è¿‡ 30 ç§’çš„ä½é‡è¦æ€§è®°å¿†ä¼šå˜æ¨¡ç³Š
        now = datetime.now()
        valid_memories = []

        for entry in self.memory_buffer:
            age = (now - entry.timestamp).total_seconds()
            decay_factor = np.exp(-age / (30 * entry.importance))  # é‡è¦çš„è¡°å‡æ…¢

            if decay_factor > 0.1:  # è¿˜èƒ½è®°èµ·æ¥
                entry.access_count += 1  # è®¿é—®åå¢å¼ºè®°å¿†
                valid_memories.append(entry)

        # å¦‚æœæœ‰æŸ¥è¯¢ï¼Œåšç›¸ä¼¼åº¦æ’åº
        if query:
            valid_memories.sort(
                key=lambda m: self._similarity(query, m.content),
                reverse=True
            )

        return valid_memories

    def get_recent_actions(self, n=5) -> List[str]:
        """è·å–æœ€è¿‘ N æ­¥æ“ä½œ"""
        return [m.content for m in list(self.memory_buffer)[-n:]]

    def update_context(self, key: str, value: Any):
        """æ›´æ–°ä¸Šä¸‹æ–‡å¿«ç…§ï¼ˆç±»ä¼¼äººç±»çš„"å½“å‰æ³¨æ„ç„¦ç‚¹"ï¼‰"""
        self.context_snapshot[key] = {
            "value": value,
            "updated_at": datetime.now()
        }

    def get_context_summary(self) -> str:
        """
        ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆç»™å¤§æ¨¡å‹çœ‹çš„ï¼‰
        """
        summary = f"""
# å½“å‰å·¥ä½œè®°å¿†

## ä¸»è¦ç›®æ ‡
{self.current_goal}

## å­ç›®æ ‡æ ˆ
{' â†’ '.join(self.sub_goals) if self.sub_goals else '(æ— )'}

## æœ€è¿‘æ“ä½œè®°å½•ï¼ˆæœ€è¿‘ 5 æ­¥ï¼‰
{self._format_recent_actions()}

## å½“å‰ä¸Šä¸‹æ–‡
{self._format_context()}

## é‡è¦è§‚å¯Ÿ
{self._format_important_memories()}
"""
        return summary

    def _format_recent_actions(self) -> str:
        recent = self.get_recent_actions(5)
        return '\n'.join(f"{i+1}. {action}" for i, action in enumerate(recent))

    def _format_important_memories(self) -> str:
        """æå–é‡è¦è®°å¿†ï¼ˆimportance > 0.7ï¼‰"""
        important = [m for m in self.memory_buffer if m.importance > 0.7]
        return '\n'.join(f"- {m.content}" for m in important[-3:])

    def detect_loop(self, window=5) -> bool:
        """
        æ£€æµ‹æ˜¯å¦é™·å…¥å¾ªç¯ï¼ˆäººç±»ä¼šæ„è¯†åˆ°"æ€ä¹ˆåˆå›åˆ°è¿™äº†ï¼Ÿ"ï¼‰
        """
        recent = self.get_recent_actions(window)

        # æ£€æµ‹é‡å¤æ“ä½œæ¨¡å¼
        for i in range(len(recent) - 2):
            if recent[i] == recent[i+2]:  # A-B-A æ¨¡å¼
                return True

        # æ£€æµ‹ç›¸åŒå±å¹•åå¤å‡ºç°
        recent_screens = [
            m.content for m in self.memory_buffer
            if m.content.startswith("å½“å‰é¡µé¢:")
        ][-3:]

        if len(recent_screens) >= 3 and recent_screens[0] == recent_screens[2]:
            return True

        return False

    def detect_stuck(self) -> bool:
        """
        æ£€æµ‹æ˜¯å¦å¡ä½ï¼ˆäººç±»ä¼šæ„è¯†åˆ°"è¿™æ ·ä¸å¯¹ï¼Œå¾—æ¢ä¸ªæ€è·¯"ï¼‰
        """

        # æœ€è¿‘ 5 æ­¥éƒ½æ²¡æœ‰è¿›å±•
        recent = self.get_recent_actions(5)
        if len(recent) >= 5:
            # éƒ½æ˜¯å¤±è´¥/é‡è¯•çš„è®°å½•
            failure_keywords = ["å¤±è´¥", "æœªæ‰¾åˆ°", "æ— æ•ˆ", "é”™è¯¯"]
            failure_count = sum(
                1 for action in recent
                if any(kw in action for kw in failure_keywords)
            )
            return failure_count >= 3

        return False
```

---

### 2.3 æ€è€ƒä¸å†³ç­–ç³»ç»Ÿï¼ˆReasoning & Decision Makingï¼‰

#### äººç±»çš„å†³ç­–è¿‡ç¨‹

```
è§‚å¯Ÿåˆ°å½“å‰å±å¹•
    â†“
æ¿€æ´»ç›¸å…³çŸ¥è¯†
  â”œâ”€ "æœç´¢æ¡†é€šå¸¸åœ¨é¡¶éƒ¨"
  â”œâ”€ "è¿”å›æŒ‰é’®åœ¨å·¦ä¸Šè§’"
  â””â”€ "ç¡®è®¤æŒ‰é’®é€šå¸¸åœ¨å³è¾¹æˆ–åº•éƒ¨"
    â†“
ç”Ÿæˆå€™é€‰æ“ä½œ
  â”œâ”€ æ–¹æ¡ˆ A: ç‚¹å‡»æœç´¢æ¡†
  â”œâ”€ æ–¹æ¡ˆ B: ç‚¹å‡»æŸä¸ªæ¨èå•†å“
  â””â”€ æ–¹æ¡ˆ C: æ»‘åŠ¨æŸ¥çœ‹æ›´å¤š
    â†“
è¯„ä¼°æ¯ä¸ªæ–¹æ¡ˆ
  â”œâ”€ ä¸ç›®æ ‡çš„ç›¸å…³æ€§
  â”œâ”€ é£é™©ï¼ˆä¼šä¸ä¼šèµ°é”™è·¯ï¼‰
  â””â”€ æˆæœ¬ï¼ˆéœ€è¦å‡ æ­¥ï¼‰
    â†“
é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ
    â†“
æ‰§è¡Œ
```

#### AI å®ç°ï¼šæ€ç»´é“¾å†³ç­–

```python
class CognitiveDecisionMaker:
    """
    æ¨¡æ‹Ÿäººç±»è®¤çŸ¥å†³ç­–è¿‡ç¨‹
    æ ¸å¿ƒï¼šä¸æ˜¯ç›´æ¥ç”Ÿæˆæ“ä½œï¼Œè€Œæ˜¯å…ˆæ€è€ƒã€æ¨ç†ã€å†å†³ç­–
    """

    def __init__(self, llm_model):
        self.llm = llm_model  # æœ¬åœ°å¤§æ¨¡å‹ï¼ˆQwen2.5-72Bï¼‰
        self.working_memory = WorkingMemory()

    async def decide_next_action(
        self,
        perception: PerceptionResult,
        goal: str
    ) -> Action:
        """
        å†³ç­–ä¸‹ä¸€æ­¥æ“ä½œï¼ˆå®Œæ•´çš„äººç±»æ€ç»´è¿‡ç¨‹ï¼‰
        """

        # ===== æ­¥éª¤ 1: å›é¡¾å·¥ä½œè®°å¿† =====
        memory_context = self.working_memory.get_context_summary()

        # ===== æ­¥éª¤ 2: æ•´åˆå½“å‰æ„ŸçŸ¥ =====
        current_situation = self._describe_situation(perception)

        # ===== æ­¥éª¤ 3: æ£€æµ‹å¼‚å¸¸æƒ…å†µ =====
        is_looping = self.working_memory.detect_loop()
        is_stuck = self.working_memory.detect_stuck()

        # ===== æ­¥éª¤ 4: æ·±åº¦æ€è€ƒï¼ˆChain of Thoughtï¼‰ =====
        thinking_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ­£åœ¨æ“ä½œæ‰‹æœºçš„äººç±»ã€‚è¯·åŸºäºå½“å‰æƒ…å†µï¼Œæ·±åº¦æ€è€ƒä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆã€‚

# ä»»åŠ¡ç›®æ ‡
{goal}

# å·¥ä½œè®°å¿†
{memory_context}

# å½“å‰è§‚å¯Ÿ
{current_situation}

# å¯ç”¨çš„æ“ä½œé€‰é¡¹
{self._list_available_actions(perception)}

# å¼‚å¸¸æ£€æµ‹
{'âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°å¾ªç¯æ“ä½œï¼Œå¯èƒ½åœ¨åŸåœ°æ‰“è½¬ï¼' if is_looping else ''}
{'âš ï¸ è­¦å‘Šï¼šæœ€è¿‘å‡ æ­¥éƒ½å¤±è´¥äº†ï¼Œå¯èƒ½å½“å‰æ€è·¯ä¸å¯¹ï¼' if is_stuck else ''}

---

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼ˆ**è¯¦ç»†å±•å¼€ä½ çš„æ€ç»´è¿‡ç¨‹**ï¼‰ï¼š

## 1. å½“å‰çŠ¶æ€åˆ†æ
- æˆ‘ç°åœ¨åœ¨å“ªé‡Œï¼Ÿï¼ˆå“ªä¸ªé¡µé¢ï¼‰
- æˆ‘åˆšæ‰åšäº†ä»€ä¹ˆï¼Ÿ
- å½“å‰é¡µé¢æœ‰ä»€ä¹ˆå…³é”®ä¿¡æ¯ï¼Ÿ

## 2. ç›®æ ‡å·®è·åˆ†æ
- æˆ‘çš„ç›®æ ‡æ˜¯ï¼š{goal}
- å½“å‰ç¦»ç›®æ ‡è¿˜æœ‰å¤šè¿œï¼Ÿ
- éœ€è¦ç»è¿‡å“ªäº›æ­¥éª¤æ‰èƒ½è¾¾æˆç›®æ ‡ï¼Ÿ

## 3. ç”Ÿæˆå€™é€‰æ–¹æ¡ˆï¼ˆè‡³å°‘ 3 ä¸ªï¼‰
ä¸ºæ¯ä¸ªæ–¹æ¡ˆè¯„ä¼°ï¼š
- æ–¹æ¡ˆæè¿°
- é¢„æœŸæ•ˆæœ
- æˆåŠŸæ¦‚ç‡
- é£é™©
- æ‰€éœ€æ­¥æ•°

## 4. æ–¹æ¡ˆæ¯”è¾ƒä¸é€‰æ‹©
åŸºäºä»¥ä¸Šåˆ†æï¼Œé€‰æ‹©æœ€ä½³æ–¹æ¡ˆï¼Œå¹¶è¯´æ˜ç†ç”±

## 5. æœ€ç»ˆå†³ç­–
å…·ä½“çš„ä¸‹ä¸€æ­¥æ“ä½œï¼ˆç‚¹å‡»/æ»‘åŠ¨/è¾“å…¥/è¿”å›ï¼‰

## 6. é¢„æœŸç»“æœ
æ‰§è¡Œååº”è¯¥çœ‹åˆ°ä»€ä¹ˆç”»é¢ï¼Ÿå¦‚ä½•åˆ¤æ–­æˆåŠŸï¼Ÿ

---

**é‡è¦**ï¼š
- å¦‚æœæ£€æµ‹åˆ°å¾ªç¯ï¼Œåº”è¯¥å°è¯•ä¸åŒçš„è·¯å¾„
- å¦‚æœè¿ç»­å¤±è´¥ï¼Œåº”è¯¥è€ƒè™‘å›é€€æˆ–é‡æ–°è§„åˆ’
- åƒäººç±»ä¸€æ ·ï¼ŒåŸºäºç›´è§‰å’Œç»éªŒåšåˆ¤æ–­
- ä¸è¦ç”Ÿæˆæœºæ¢°çš„æ“ä½œåºåˆ—ï¼Œè€Œæ˜¯åŸºäºå½“å‰ç”»é¢çµæ´»å†³ç­–
"""

        # è°ƒç”¨å¤§æ¨¡å‹æ€è€ƒï¼ˆä¸é™åˆ¶ tokenï¼Œè®©å®ƒå……åˆ†æ€è€ƒï¼‰
        thinking_response = await self.llm.generate(
            prompt=thinking_prompt,
            max_tokens=3000,  # å…è®¸è¯¦ç»†çš„æ€ç»´é“¾
            temperature=0.3   # ç¨é«˜çš„æ¸©åº¦ï¼Œæ¨¡æ‹Ÿäººç±»çš„åˆ›é€ æ€§æ€ç»´
        )

        # ===== æ­¥éª¤ 5: ä»æ€è€ƒä¸­æå–å†³ç­– =====
        decision = self._extract_decision(thinking_response)

        # ===== æ­¥éª¤ 6: äºŒæ¬¡éªŒè¯ï¼ˆäººç±»çš„"å†æƒ³ä¸€æƒ³"ï¼‰ =====
        if decision.risk_level > 0.7:
            verification = await self._verify_risky_decision(
                decision,
                thinking_response,
                perception
            )
            if not verification.confirmed:
                # é‡æ–°æ€è€ƒ
                return await self._rethink(perception, goal, verification.reason)

        # ===== æ­¥éª¤ 7: è®°å½•åˆ°å·¥ä½œè®°å¿† =====
        self.working_memory.add_memory(
            f"å†³å®šæ‰§è¡Œ: {decision.description}",
            importance=0.6
        )
        self.working_memory.add_memory(
            f"æœŸæœ›ç»“æœ: {decision.expected_outcome}",
            importance=0.5
        )

        return decision.to_action()

    async def _verify_risky_decision(self, decision, thinking, perception):
        """
        äºŒæ¬¡éªŒè¯é«˜é£é™©å†³ç­–ï¼ˆæ¨¡æ‹Ÿäººç±»çš„"ç¡®è®¤ä¸€ä¸‹"ï¼‰
        """

        verify_prompt = f"""
åˆšæ‰çš„æ€è€ƒå¾—å‡ºç»“è®ºï¼š{decision.description}

ä½†è¿™ä¸ªæ“ä½œé£é™©è¾ƒé«˜ï¼ˆ{decision.risk_level:.0%}ï¼‰ã€‚

è¯·å†æ¬¡ç¡®è®¤ï¼š
1. è¿™ä¸ªæ“ä½œçœŸçš„æ˜¯æœ€ä¼˜é€‰æ‹©å—ï¼Ÿ
2. æœ‰æ²¡æœ‰æ›´å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Ÿ
3. å¦‚æœå¤±è´¥äº†æ€ä¹ˆåŠï¼Ÿæœ‰å›é€€è·¯å¾„å—ï¼Ÿ

å½“å‰å±å¹•æˆªå›¾ï¼š[é™„ä¸Š]
åˆšæ‰çš„å®Œæ•´æ€è€ƒï¼š{thinking}

è¯·ç»™å‡ºæ˜ç¡®çš„ç¡®è®¤/å¦å†³æ„è§ã€‚
"""

        response = await self.llm.generate(verify_prompt, max_tokens=1000)
        return self._parse_verification(response)
```

---

### 2.4 è¯•é”™ä¸å³æ—¶çº é”™ç³»ç»Ÿï¼ˆTrial-and-Error & Immediate Correctionï¼‰

#### äººç±»è¯•é”™ç‰¹ç‚¹

```
æ‰§è¡Œæ“ä½œ
    â†“
è§‚å¯Ÿç»“æœï¼ˆ0.5 ç§’å†…ï¼‰
    â†“
å¿«é€Ÿåˆ¤æ–­ï¼š
  â”œâ”€ âœ… "å¯¹äº†ï¼Œé¡µé¢è·³è½¬äº†" â†’ ç»§ç»­
  â”œâ”€ âš ï¸ "å’¦ï¼Œæ€ä¹ˆæ²¡ååº”ï¼Ÿ" â†’ å†ç‚¹ä¸€æ¬¡æˆ–æ¢åœ°æ–¹ç‚¹
  â”œâ”€ âŒ "ç³Ÿç³•ï¼Œç‚¹é”™äº†" â†’ ç«‹å³æŒ‰è¿”å›é”®
  â””â”€ â“ "ä¸ç¡®å®šï¼Œå†çœ‹çœ‹" â†’ ç­‰å¾…/è§‚å¯Ÿ
```

#### AI å®ç°ï¼šå®æ—¶åé¦ˆå¾ªç¯

```python
class TrialAndErrorController:
    """
    è¯•é”™æ§åˆ¶å™¨
    ç‰¹ç‚¹ï¼š
    1. æ‰§è¡Œåç«‹å³è§‚å¯Ÿ
    2. å¿«é€Ÿåˆ¤æ–­å¯¹é”™
    3. é”™è¯¯å³æ—¶çº æ­£
    4. å­¦ä¹ å¤±è´¥æ¨¡å¼
    """

    def __init__(self):
        self.vl_model = VLModel("Qwen2-VL-72B")
        self.error_patterns = []  # è®°å½•å¤±è´¥æ¨¡å¼

    async def execute_with_feedback(
        self,
        action: Action,
        device: AndroidDevice,
        expected_outcome: str
    ) -> ExecutionResult:
        """
        æ‰§è¡Œæ“ä½œå¹¶å³æ—¶åé¦ˆ
        """

        # 1. è®°å½•æ‰§è¡Œå‰çŠ¶æ€
        screenshot_before = device.screenshot()

        # 2. æ‰§è¡Œæ“ä½œ
        start_time = time.time()
        device.execute(action)

        # 3. ç­‰å¾…é¡µé¢ç¨³å®šï¼ˆæ¨¡æ‹Ÿäººç±»ååº”æ—¶é—´ï¼‰
        await asyncio.sleep(0.5)  # äººç±»ååº”æ—¶é—´çº¦ 200-500ms

        # 4. è§‚å¯Ÿæ‰§è¡ŒåçŠ¶æ€
        screenshot_after = device.screenshot()
        execution_time = time.time() - start_time

        # 5. ç«‹å³åˆ¤æ–­ï¼šè¿™ä¸€æ­¥å¯¹ä¸å¯¹ï¼Ÿ
        judgment = await self._immediate_judgment(
            screenshot_before=screenshot_before,
            screenshot_after=screenshot_after,
            action=action,
            expected_outcome=expected_outcome
        )

        # 6. æ ¹æ®åˆ¤æ–­ç»“æœé‡‡å–è¡ŒåŠ¨
        if judgment.status == "success":
            return ExecutionResult(
                success=True,
                actual_outcome=judgment.actual_outcome,
                confidence=judgment.confidence
            )

        elif judgment.status == "wrong_action":
            # ç«‹å³çº é”™ï¼ˆæŒ‰è¿”å›é”®ï¼‰
            logger.warning(f"âŒ æ“ä½œé”™è¯¯: {judgment.reason}ï¼Œç«‹å³è¿”å›")
            device.press_back()
            await asyncio.sleep(0.3)

            # è®°å½•å¤±è´¥æ¨¡å¼
            self._learn_failure_pattern(action, judgment.reason)

            return ExecutionResult(
                success=False,
                error="wrong_action",
                corrective_action="pressed_back",
                reason=judgment.reason
            )

        elif judgment.status == "no_effect":
            # æ²¡ååº”ï¼Œå¯èƒ½éœ€è¦é‡è¯•
            logger.warning(f"âš ï¸ æ“ä½œæ— æ•ˆ: {judgment.reason}")

            # åˆ¤æ–­æ˜¯å¦è¦é‡è¯•
            if self._should_retry(action):
                logger.info("ğŸ”„ é‡è¯•æ“ä½œ")
                # ç¨å¾®è°ƒæ•´ä½ç½®é‡è¯•ï¼ˆæ¨¡æ‹Ÿäººç±»"æ¢ä¸ªåœ°æ–¹ç‚¹"ï¼‰
                adjusted_action = self._adjust_action(action, screenshot_after)
                return await self.execute_with_feedback(
                    adjusted_action,
                    device,
                    expected_outcome
                )
            else:
                return ExecutionResult(
                    success=False,
                    error="no_effect",
                    reason=judgment.reason
                )

        elif judgment.status == "uncertain":
            # ä¸ç¡®å®šï¼Œå¤šè§‚å¯Ÿä¸€ä¼šå„¿
            logger.info("â“ ç»“æœä¸ç¡®å®šï¼Œç»§ç»­è§‚å¯Ÿ...")
            await asyncio.sleep(1.0)

            # å†æ¬¡åˆ¤æ–­
            screenshot_final = device.screenshot()
            final_judgment = await self._delayed_judgment(
                screenshot_before,
                screenshot_final,
                expected_outcome
            )

            return ExecutionResult(
                success=final_judgment.status == "success",
                actual_outcome=final_judgment.actual_outcome,
                confidence=final_judgment.confidence
            )

    async def _immediate_judgment(
        self,
        screenshot_before: Image,
        screenshot_after: Image,
        action: Action,
        expected_outcome: str
    ) -> Judgment:
        """
        å³æ—¶åˆ¤æ–­ï¼ˆæ¨¡æ‹Ÿäººç±»çš„å¿«é€Ÿååº”ï¼‰
        ä½¿ç”¨è§†è§‰æ¨¡å‹å¯¹æ¯”å‰åæˆªå›¾
        """

        # å¿«é€Ÿå¯¹æ¯”æ³• 1: å›¾åƒå·®å¼‚
        diff_ratio = self._compute_image_diff(screenshot_before, screenshot_after)

        if diff_ratio < 0.05:  # å‡ ä¹æ²¡å˜åŒ–
            return Judgment(
                status="no_effect",
                reason="å±å¹•å‡ ä¹æ— å˜åŒ–ï¼Œæ“ä½œå¯èƒ½æ— æ•ˆ",
                confidence=0.8
            )

        # æ·±åº¦åˆ¤æ–­æ³• 2: è§†è§‰æ¨¡å‹åˆ†æï¼ˆä¸é™ tokenï¼‰
        prompt = f"""
å¯¹æ¯”è¿™ä¸¤å¼ æˆªå›¾ï¼ˆæ“ä½œå‰ vs æ“ä½œåï¼‰ï¼Œå¿«é€Ÿåˆ¤æ–­æ“ä½œç»“æœï¼š

æ“ä½œå†…å®¹ï¼š{action.description}
æœŸæœ›ç»“æœï¼š{expected_outcome}

è¯·å›ç­”ï¼š
1. å±å¹•å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ï¼Ÿ
2. è¿™ä¸ªå˜åŒ–ç¬¦åˆé¢„æœŸå—ï¼Ÿ
3. åˆ¤æ–­ï¼šæˆåŠŸ/å¤±è´¥/ä¸ç¡®å®š

åˆ†ç±»æ ‡å‡†ï¼š
- æˆåŠŸï¼šé¡µé¢è·³è½¬ç¬¦åˆé¢„æœŸï¼Œæˆ–ç›®æ ‡å…ƒç´ å‡ºç°
- å¤±è´¥ï¼šé¡µé¢è·³è½¬åˆ°é”™è¯¯é¡µé¢ï¼Œæˆ–å‡ºç°é”™è¯¯æç¤º
- ä¸ç¡®å®šï¼šæœ‰å˜åŒ–ä½†ä¸æ¸…æ¥šæ˜¯å¦æ­£ç¡®
- æ— æ•ˆï¼šå‡ ä¹æ— å˜åŒ–

ç»™å‡ºç®€æ´çš„åˆ¤æ–­ç»“æœã€‚
"""

        response = await self.vl_model.analyze_multi_image(
            images=[screenshot_before, screenshot_after],
            prompt=prompt,
            max_tokens=500
        )

        return self._parse_judgment(response)

    def _learn_failure_pattern(self, action: Action, reason: str):
        """
        å­¦ä¹ å¤±è´¥æ¨¡å¼ï¼ˆé¿å…é‡å¤çŠ¯é”™ï¼‰
        """

        pattern = FailurePattern(
            action_type=action.type,
            target_description=action.target,
            failure_reason=reason,
            timestamp=datetime.now()
        )

        self.error_patterns.append(pattern)

        # å¦‚æœåŒæ ·çš„é”™è¯¯å‡ºç° 3 æ¬¡ï¼Œè®°å…¥é•¿æœŸè®°å¿†ï¼ˆæ°¸ä¹…é¿å…ï¼‰
        similar_failures = [
            p for p in self.error_patterns
            if p.is_similar_to(pattern)
        ]

        if len(similar_failures) >= 3:
            logger.warning(f"ğŸš« æ£€æµ‹åˆ°é‡å¤å¤±è´¥æ¨¡å¼ï¼Œè®°å…¥é•¿æœŸè®°å¿†: {pattern}")
            # å­˜å…¥é•¿æœŸè®°å¿†æ•°æ®åº“
            self._save_to_long_term_memory(pattern)

    def _should_retry(self, action: Action) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•ï¼ˆåŸºäºå¤±è´¥å†å²ï¼‰
        """

        # æ£€æŸ¥æœ€è¿‘æ˜¯å¦é‡è¯•è¿‡åŒæ ·çš„æ“ä½œ
        recent_actions = self.working_memory.get_recent_actions(3)

        retry_count = sum(
            1 for a in recent_actions
            if a.startswith(f"é‡è¯•: {action.type}")
        )

        # äººç±»é€šå¸¸ä¸ä¼šé‡è¯•è¶…è¿‡ 2 æ¬¡
        return retry_count < 2

    def _adjust_action(self, action: Action, screenshot: Image) -> Action:
        """
        è°ƒæ•´æ“ä½œï¼ˆæ¨¡æ‹Ÿäººç±»"æ¢ä¸ªåœ°æ–¹ç‚¹"ï¼‰
        """

        if action.type == "click":
            # åœ¨ç›®æ ‡åŒºåŸŸå‘¨å›´éšæœºåç§»ä¸€ç‚¹
            offset_x = random.randint(-10, 10)
            offset_y = random.randint(-10, 10)

            adjusted = action.copy()
            adjusted.x += offset_x
            adjusted.y += offset_y

            logger.info(f"ğŸ”§ è°ƒæ•´ç‚¹å‡»ä½ç½®: ({action.x}, {action.y}) â†’ ({adjusted.x}, {adjusted.y})")

            return adjusted

        return action
```

---

### 2.5 æŒç»­è§‚å¯Ÿå¾ªç¯ï¼ˆContinuous Observation Loopï¼‰

#### äººç±»çš„æŒç»­è§‚å¯Ÿæ¨¡å¼

```
äººç±»ä¸æ˜¯"æ‰§è¡Œå®Œå°±ä¸ç®¡"ï¼Œè€Œæ˜¯ï¼š

æ‰§è¡Œæ“ä½œ â†’ çœ‹ç€å±å¹• â†’ çœ‹ç€å±å¹• â†’ çœ‹ç€å±å¹• â†’ ç¡®è®¤åˆ°ä½äº† â†’ ä¸‹ä¸€æ­¥

ç‰¹ç‚¹ï¼š
1. æŒç»­ç›‘æ§ï¼ˆæ¯ 0.5-1 ç§’æ‰«ä¸€çœ¼ï¼‰
2. åŠ¨æ€è°ƒæ•´ï¼ˆå‘ç°ä¸å¯¹ç«‹å³æ”¹ï¼‰
3. ç­‰å¾…åŠ è½½ï¼ˆçœ‹åˆ°è½¬åœˆåœˆä¼šç­‰ï¼‰
4. è¯†åˆ«å¹²æ‰°ï¼ˆå¼¹çª—ã€å¹¿å‘Šä¼šå…ˆå…³æ‰ï¼‰
```

#### AI å®ç°ï¼šäº‹ä»¶é©±åŠ¨è§‚å¯Ÿ

```python
class ContinuousObserver:
    """
    æŒç»­è§‚å¯Ÿå™¨
    æ¨¡æ‹Ÿäººç±»çš„"ä¸€ç›´ç›¯ç€å±å¹•çœ‹"
    """

    def __init__(self, device: AndroidDevice):
        self.device = device
        self.vl_model = VLModel("Qwen2-VL-72B")
        self.observation_interval = 0.5  # æ¯ 0.5 ç§’è§‚å¯Ÿä¸€æ¬¡
        self.observers = []  # è§‚å¯Ÿå›è°ƒ

    async def start_observing(self, duration: float = 30):
        """
        å¯åŠ¨æŒç»­è§‚å¯Ÿï¼ˆå¼‚æ­¥åå°è¿è¡Œï¼‰
        """

        start_time = time.time()

        while time.time() - start_time < duration:
            # æ•è·å½“å‰å±å¹•
            screenshot = self.device.screenshot()

            # å¿«é€Ÿæ£€æµ‹å…³é”®äº‹ä»¶
            events = await self._detect_events(screenshot)

            # è§¦å‘äº‹ä»¶å¤„ç†
            for event in events:
                await self._handle_event(event)

            # ç­‰å¾…ä¸‹ä¸€æ¬¡è§‚å¯Ÿ
            await asyncio.sleep(self.observation_interval)

    async def _detect_events(self, screenshot: Image) -> List[Event]:
        """
        æ£€æµ‹å±å¹•äº‹ä»¶ï¼ˆæ¨¡æ‹Ÿäººç±»çš„"æ³¨æ„åˆ°"ï¼‰
        """

        events = []

        # 1. æ£€æµ‹åŠ è½½çŠ¶æ€
        if self._is_loading(screenshot):
            events.append(Event(type="loading", data=None))

        # 2. æ£€æµ‹å¼¹çª—
        popup = await self._detect_popup(screenshot)
        if popup:
            events.append(Event(type="popup_appeared", data=popup))

        # 3. æ£€æµ‹é¡µé¢è·³è½¬
        if self._page_changed(screenshot):
            events.append(Event(type="page_changed", data=screenshot))

        # 4. æ£€æµ‹é”™è¯¯æç¤º
        error = await self._detect_error_message(screenshot)
        if error:
            events.append(Event(type="error_appeared", data=error))

        # 5. æ£€æµ‹ç›®æ ‡å…ƒç´ å‡ºç°
        target = await self._detect_target_element(screenshot)
        if target:
            events.append(Event(type="target_found", data=target))

        return events

    async def _handle_event(self, event: Event):
        """
        å¤„ç†æ£€æµ‹åˆ°çš„äº‹ä»¶ï¼ˆæ¨¡æ‹Ÿäººç±»çš„å³æ—¶ååº”ï¼‰
        """

        if event.type == "loading":
            logger.info("â³ æ£€æµ‹åˆ°åŠ è½½ä¸­ï¼Œç­‰å¾…...")
            await self._wait_for_loading_complete()

        elif event.type == "popup_appeared":
            logger.info(f"ğŸ”” æ£€æµ‹åˆ°å¼¹çª—: {event.data.title}")

            # åˆ¤æ–­æ˜¯å¦éœ€è¦å…³é—­
            should_close = await self._should_close_popup(event.data)

            if should_close:
                logger.info("âŒ å…³é—­å¼¹çª—")
                await self._close_popup(event.data)

        elif event.type == "error_appeared":
            logger.error(f"â— æ£€æµ‹åˆ°é”™è¯¯: {event.data.message}")
            # è§¦å‘é”™è¯¯å¤„ç†æµç¨‹
            await self._handle_error(event.data)

        elif event.type == "target_found":
            logger.info(f"ğŸ¯ å‘ç°ç›®æ ‡å…ƒç´ : {event.data}")
            # é€šçŸ¥ä¸»æµç¨‹
            await self._notify_target_found(event.data)

    async def _detect_popup(self, screenshot: Image) -> Optional[PopupInfo]:
        """
        æ£€æµ‹å¼¹çª—ï¼ˆäººç±»èƒ½å¿«é€Ÿè¯†åˆ«å¼¹çª—ï¼‰
        """

        # æ–¹æ³• 1: è§†è§‰ç‰¹å¾æ£€æµ‹ï¼ˆå¿«é€Ÿï¼‰
        # å¼¹çª—é€šå¸¸æœ‰ï¼šåŠé€æ˜èƒŒæ™¯ã€å±…ä¸­çŸ©å½¢ã€å…³é—­æŒ‰é’®

        # æ–¹æ³• 2: UI å…ƒç´ æ£€æµ‹
        ui_elements = self.device.dump_hierarchy()
        for elem in ui_elements:
            if elem.type == "Dialog" or "popup" in elem.class_name.lower():
                return PopupInfo(
                    title=elem.text,
                    bounds=elem.bounds,
                    close_button=self._find_close_button(elem)
                )

        # æ–¹æ³• 3: è§†è§‰æ¨¡å‹åˆ¤æ–­ï¼ˆå‡†ç¡®ä½†æ…¢ï¼‰
        prompt = "è¿™ä¸ªå±å¹•ä¸Šæœ‰å¼¹çª—å—ï¼Ÿå¦‚æœæœ‰ï¼Œæè¿°å¼¹çª—çš„ä½ç½®å’Œå†…å®¹ã€‚"
        response = await self.vl_model.analyze(screenshot, prompt, max_tokens=200)

        if "æœ‰å¼¹çª—" in response or "popup" in response.lower():
            return self._parse_popup_info(response)

        return None

    async def _should_close_popup(self, popup: PopupInfo) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å…³é—­å¼¹çª—ï¼ˆæ¨¡æ‹Ÿäººç±»çš„åˆ¤æ–­ï¼‰
        """

        # ç­–ç•¥ï¼š
        # 1. å¹¿å‘Šå¼¹çª— â†’ å…³é—­
        # 2. æƒé™è¯·æ±‚ â†’ æ ¹æ®éœ€è¦å†³å®š
        # 3. é‡è¦é€šçŸ¥ â†’ å¯èƒ½è¦é˜…è¯»

        prompt = f"""
å‡ºç°äº†ä¸€ä¸ªå¼¹çª—ï¼š{popup.title}

åˆ¤æ–­ï¼š
1. è¿™æ˜¯å¹¿å‘Š/è¥é”€å¼¹çª—å—ï¼Ÿï¼ˆæ˜¯ â†’ ç›´æ¥å…³é—­ï¼‰
2. è¿™æ˜¯æƒé™è¯·æ±‚å—ï¼Ÿï¼ˆå¦‚æœä¸ä»»åŠ¡æ— å…³ â†’ å…³é—­æˆ–æ‹’ç»ï¼‰
3. è¿™æ˜¯é‡è¦ä¿¡æ¯å—ï¼Ÿï¼ˆæ˜¯ â†’ å¯èƒ½éœ€è¦é˜…è¯»æˆ–ç¡®è®¤ï¼‰

ç»™å‡ºå»ºè®®ï¼šå…³é—­/é˜…è¯»/ç¡®è®¤/æ‹’ç»
"""

        response = await self.llm.generate(prompt, max_tokens=100)

        return "å…³é—­" in response or "æ‹’ç»" in response

    async def _wait_for_loading_complete(self, timeout=10):
        """
        ç­‰å¾…åŠ è½½å®Œæˆï¼ˆæ¨¡æ‹Ÿäººç±»"çœ‹åˆ°è½¬åœˆå°±ç­‰"ï¼‰
        """

        start_time = time.time()

        while time.time() - start_time < timeout:
            screenshot = self.device.screenshot()

            if not self._is_loading(screenshot):
                logger.info("âœ… åŠ è½½å®Œæˆ")
                return True

            await asyncio.sleep(0.5)

        logger.warning("â° åŠ è½½è¶…æ—¶")
        return False

    def _is_loading(self, screenshot: Image) -> bool:
        """
        æ£€æµ‹åŠ è½½çŠ¶æ€ï¼ˆè¯†åˆ«è½¬åœˆåœˆã€è¿›åº¦æ¡ï¼‰
        """

        # æ–¹æ³• 1: æ£€æµ‹ UI å…ƒç´ 
        ui = self.device.dump_hierarchy()
        for elem in ui:
            if elem.type == "ProgressBar" or "loading" in elem.id.lower():
                return True

        # æ–¹æ³• 2: è§†è§‰æ£€æµ‹ï¼ˆæ£€æµ‹æ—‹è½¬åŠ¨ç”»ï¼‰
        # å¯ä»¥ç”¨å…‰æµæ³•æ£€æµ‹æ—‹è½¬è¿åŠ¨

        # æ–¹æ³• 3: OCR æ£€æµ‹"åŠ è½½ä¸­"æ–‡å­—
        ocr_result = self.ocr.extract_text(screenshot)
        if any(kw in ocr_result for kw in ["åŠ è½½ä¸­", "Loading", "è¯·ç¨å€™"]):
            return True

        return False
```

---

## ä¸‰ã€å®Œæ•´çš„äººç±»è®¤çŸ¥æ¨¡æ‹Ÿæ¶æ„

### 3.1 æ€»ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Human-Like Agent                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Goal Manager  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤   User Input   â”‚         â”‚
â”‚  â”‚  (ç›®æ ‡ç®¡ç†)     â”‚         â”‚   (ä»»åŠ¡è¾“å…¥)    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                                              â”‚
â”‚           â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚       Working Memory (å·¥ä½œè®°å¿†)          â”‚           â”‚
â”‚  â”‚  - å½“å‰ç›®æ ‡                               â”‚           â”‚
â”‚  â”‚  - å­ç›®æ ‡æ ˆ                               â”‚           â”‚
â”‚  â”‚  - æœ€è¿‘æ“ä½œ (deque, 7Â±2 å®¹é‡)            â”‚           â”‚
â”‚  â”‚  - ä¸Šä¸‹æ–‡å¿«ç…§                             â”‚           â”‚
â”‚  â”‚  - å¾ªç¯æ£€æµ‹                               â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚      Perception System (æ„ŸçŸ¥ç³»ç»Ÿ)        â”‚           â”‚
â”‚  â”‚                                           â”‚           â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚           â”‚
â”‚  â”‚  â”‚ Vision Model â”‚  â”‚ UI Detector  â”‚      â”‚           â”‚
â”‚  â”‚  â”‚ (Qwen2-VL)  â”‚  â”‚ (UI å…ƒç´ )    â”‚      â”‚           â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚           â”‚
â”‚  â”‚         â”‚                  â”‚              â”‚           â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚           â”‚
â”‚  â”‚                  â–¼                        â”‚           â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚           â”‚
â”‚  â”‚         â”‚  Attention Map   â”‚             â”‚           â”‚
â”‚  â”‚         â”‚  (æ³¨æ„åŠ›çƒ­åŠ›å›¾)   â”‚             â”‚           â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Cognitive Decision Maker (è®¤çŸ¥å†³ç­–)    â”‚           â”‚
â”‚  â”‚                                           â”‚           â”‚
â”‚  â”‚  è¾“å…¥: æ„ŸçŸ¥ç»“æœ + å·¥ä½œè®°å¿†                â”‚           â”‚
â”‚  â”‚  è¿‡ç¨‹:                                    â”‚           â”‚
â”‚  â”‚    1. å›é¡¾è®°å¿†                            â”‚           â”‚
â”‚  â”‚    2. åˆ†æç°çŠ¶                            â”‚           â”‚
â”‚  â”‚    3. ç”Ÿæˆå€™é€‰æ–¹æ¡ˆ (3-5 ä¸ª)              â”‚           â”‚
â”‚  â”‚    4. è¯„ä¼° (ç›¸å…³æ€§/é£é™©/æˆæœ¬)            â”‚           â”‚
â”‚  â”‚    5. é€‰æ‹©æœ€ä½³                            â”‚           â”‚
â”‚  â”‚    6. äºŒæ¬¡éªŒè¯ (é«˜é£é™©æ—¶)                â”‚           â”‚
â”‚  â”‚  è¾“å‡º: å†³ç­–è¡ŒåŠ¨                           â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Trial-and-Error Controller (è¯•é”™)     â”‚           â”‚
â”‚  â”‚                                           â”‚           â”‚
â”‚  â”‚  æ‰§è¡Œå‰: è®°å½•çŠ¶æ€                         â”‚           â”‚
â”‚  â”‚  æ‰§è¡Œ: å‘é€æ“ä½œæŒ‡ä»¤                       â”‚           â”‚
â”‚  â”‚  æ‰§è¡Œå (0.5s):                          â”‚           â”‚
â”‚  â”‚    - å³æ—¶åˆ¤æ–­ (æˆåŠŸ/å¤±è´¥/ä¸ç¡®å®š)         â”‚           â”‚
â”‚  â”‚    - å¤±è´¥ â†’ ç«‹å³å›é€€                     â”‚           â”‚
â”‚  â”‚    - æ— æ•ˆ â†’ è°ƒæ•´é‡è¯•                     â”‚           â”‚
â”‚  â”‚    - ä¸ç¡®å®š â†’ ç»§ç»­è§‚å¯Ÿ                   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Continuous Observer (æŒç»­è§‚å¯Ÿ)          â”‚           â”‚
â”‚  â”‚                                           â”‚           â”‚
â”‚  â”‚  åå°ä»»åŠ¡ (0.5s é—´éš”):                   â”‚           â”‚
â”‚  â”‚    - æ£€æµ‹åŠ è½½ â†’ ç­‰å¾…                     â”‚           â”‚
â”‚  â”‚    - æ£€æµ‹å¼¹çª— â†’ å…³é—­                     â”‚           â”‚
â”‚  â”‚    - æ£€æµ‹é”™è¯¯ â†’ è§¦å‘å¤„ç†                 â”‚           â”‚
â”‚  â”‚    - æ£€æµ‹ç›®æ ‡ â†’ é€šçŸ¥ä¸»æµç¨‹               â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                     â”‚                                    â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚           â–¼                   â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ æ›´æ–°å·¥ä½œè®°å¿†  â”‚    â”‚ å­¦ä¹ å¤±è´¥æ¨¡å¼  â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                          â”‚
â”‚  å¾ªç¯: è§‚å¯Ÿ â†’ æ€è€ƒ â†’ å†³ç­– â†’ æ‰§è¡Œ â†’ åé¦ˆ â†’ è§‚å¯Ÿ...      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ä¸»æ§å¾ªç¯å®ç°

```python
class HumanLikeAgent:
    """
    äººç±»è®¤çŸ¥æ¨¡æ‹Ÿ Agent
    æ ¸å¿ƒï¼šåƒäººä¸€æ ·è§‚å¯Ÿã€æ€è€ƒã€å°è¯•ã€çº é”™
    """

    def __init__(self, device: AndroidDevice):
        # è®¾å¤‡æ§åˆ¶
        self.device = device

        # è®¤çŸ¥æ¨¡å—
        self.vision = HumanLikeVisionSystem()
        self.working_memory = WorkingMemory(capacity=7)
        self.decision_maker = CognitiveDecisionMaker(llm=local_llm)
        self.trial_controller = TrialAndErrorController()
        self.observer = ContinuousObserver(device)

        # é•¿æœŸè®°å¿†ï¼ˆå¯é€‰ï¼‰
        self.long_term_memory = LongTermMemoryStore()

    async def execute_task(self, task: str, max_steps: int = 50):
        """
        æ‰§è¡Œä»»åŠ¡ï¼ˆä¸»å¾ªç¯ï¼‰
        """

        # 1. åˆå§‹åŒ–
        self.working_memory.set_goal(task)
        logger.info(f"ğŸ¯ å¼€å§‹ä»»åŠ¡: {task}")

        # 2. å¯åŠ¨æŒç»­è§‚å¯Ÿï¼ˆåå°ï¼‰
        observer_task = asyncio.create_task(
            self.observer.start_observing(duration=300)  # æœ€å¤šè§‚å¯Ÿ 5 åˆ†é’Ÿ
        )

        # 3. ä¸»å¾ªç¯
        for step in range(max_steps):
            logger.info(f"\n{'='*60}")
            logger.info(f"æ­¥éª¤ {step + 1}/{max_steps}")
            logger.info(f"{'='*60}")

            # ===== æ­¥éª¤ 1: è§‚å¯Ÿï¼ˆVisionï¼‰ =====
            screenshot = self.device.screenshot()
            perception = await self.vision.perceive_screen(screenshot)

            logger.info(f"ğŸ‘€ è§‚å¯Ÿ: {perception.screen_type}")
            logger.info(f"ğŸ” å…³é”®å…ƒç´ : {len(perception.actionable_items)} ä¸ª")

            # æ›´æ–°å·¥ä½œè®°å¿†
            self.working_memory.update_context("current_screen", perception.screen_type)
            self.working_memory.add_memory(
                f"å½“å‰é¡µé¢: {perception.screen_type}",
                importance=0.4
            )

            # ===== æ­¥éª¤ 2: æ£€æµ‹å¼‚å¸¸ =====
            if self.working_memory.detect_loop():
                logger.warning("ğŸ”„ æ£€æµ‹åˆ°å¾ªç¯ï¼å°è¯•ä¸åŒè·¯å¾„")
                await self._break_loop(perception)

            if self.working_memory.detect_stuck():
                logger.warning("ğŸš« æ£€æµ‹åˆ°å¡ä½ï¼é‡æ–°è§„åˆ’")
                await self._replan(task)

            # ===== æ­¥éª¤ 3: å†³ç­–ï¼ˆCognitionï¼‰ =====
            decision = await self.decision_maker.decide_next_action(
                perception=perception,
                goal=task
            )

            logger.info(f"ğŸ’­ å†³ç­–: {decision.description}")
            logger.info(f"ğŸ² ç½®ä¿¡åº¦: {decision.confidence:.0%}")

            # ===== æ­¥éª¤ 4: æ‰§è¡Œ + å³æ—¶åé¦ˆï¼ˆTrial-and-Errorï¼‰ =====
            result = await self.trial_controller.execute_with_feedback(
                action=decision.action,
                device=self.device,
                expected_outcome=decision.expected_outcome
            )

            # è®°å½•åˆ°å·¥ä½œè®°å¿†
            if result.success:
                self.working_memory.add_memory(
                    f"âœ“ {decision.description} - æˆåŠŸ",
                    importance=0.6
                )
            else:
                self.working_memory.add_memory(
                    f"âœ— {decision.description} - å¤±è´¥: {result.reason}",
                    importance=0.7
                )

            # ===== æ­¥éª¤ 5: åˆ¤æ–­æ˜¯å¦å®Œæˆä»»åŠ¡ =====
            is_complete = await self._check_task_completion(
                task=task,
                current_perception=perception
            )

            if is_complete:
                logger.info(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼å…± {step + 1} æ­¥")

                # ä¿å­˜æˆåŠŸè·¯å¾„åˆ°é•¿æœŸè®°å¿†ï¼ˆå¯é€‰ï¼‰
                await self._save_experience(task, step + 1)

                break

            # ===== æ­¥éª¤ 6: çŸ­æš‚æš‚åœï¼ˆæ¨¡æ‹Ÿäººç±»æ€è€ƒé—´éš”ï¼‰ =====
            await asyncio.sleep(0.3)  # äººç±»æ“ä½œé—´éš”çº¦ 300-500ms

        else:
            logger.error(f"âŒ ä»»åŠ¡å¤±è´¥ï¼šè¶…è¿‡æœ€å¤§æ­¥æ•° {max_steps}")

        # åœæ­¢åå°è§‚å¯Ÿ
        observer_task.cancel()

    async def _check_task_completion(self, task: str, current_perception: PerceptionResult) -> bool:
        """
        åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å®Œæˆï¼ˆéœ€è¦æ·±åº¦ç†è§£ï¼‰
        """

        prompt = f"""
ä»»åŠ¡ç›®æ ‡ï¼š{task}

å½“å‰å±å¹•çŠ¶æ€ï¼š
- é¡µé¢ç±»å‹: {current_perception.screen_type}
- å¯è§å…ƒç´ : {current_perception.key_elements}
- OCR æ–‡å­—: {current_perception.text_content[:500]}

æ“ä½œå†å²ï¼š
{self.working_memory.get_context_summary()}

åˆ¤æ–­ï¼šä»»åŠ¡æ˜¯å¦å·²ç»å®Œæˆï¼Ÿ

è¯„ä¼°æ ‡å‡†ï¼š
1. ç›®æ ‡æ˜¯å¦è¾¾æˆï¼Ÿï¼ˆå¦‚"æ‰“å¼€æ·˜å®" â†’ çœ‹åˆ°æ·˜å®é¦–é¡µï¼‰
2. æ˜¯å¦åœ¨é¢„æœŸçš„ç»ˆç‚¹é¡µé¢ï¼Ÿ
3. æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„å­æ­¥éª¤ï¼Ÿ

ç»™å‡ºæ˜ç¡®çš„åˆ¤æ–­ï¼šå·²å®Œæˆ/æœªå®Œæˆ/éƒ¨åˆ†å®Œæˆ
å¹¶è¯´æ˜ç†ç”±ã€‚
"""

        response = await self.decision_maker.llm.generate(
            prompt,
            max_tokens=500
        )

        return "å·²å®Œæˆ" in response

    async def _break_loop(self, perception: PerceptionResult):
        """
        æ‰“ç ´å¾ªç¯ï¼ˆäººç±»ä¼šå°è¯•ä¸åŒè·¯å¾„ï¼‰
        """

        # ç­–ç•¥ 1: æŒ‰è¿”å›é”®ï¼Œå›åˆ°ä¸Šä¸€çº§
        logger.info("ğŸ”™ å°è¯•è¿”å›ä¸Šä¸€çº§")
        self.device.press_back()
        await asyncio.sleep(0.5)

        # ç­–ç•¥ 2: æ ‡è®°æœ€è¿‘çš„æ“ä½œä¸º"ç¦æ­¢"ï¼Œé¿å…é‡å¤
        recent_actions = self.working_memory.get_recent_actions(3)
        for action in recent_actions:
            self.working_memory.add_memory(
                f"â›” ç¦æ­¢é‡å¤: {action}",
                importance=0.9
            )

        # ç­–ç•¥ 3: é‡æ–°è§‚å¯Ÿï¼Œå¯»æ‰¾å…¶ä»–è·¯å¾„
        # (ä¸‹ä¸€æ¬¡ decide_next_action ä¼šçœ‹åˆ°ç¦æ­¢æ ‡è®°)

    async def _replan(self, task: str):
        """
        é‡æ–°è§„åˆ’ï¼ˆäººç±»ä¼š"æ¢ä¸ªæ€è·¯"ï¼‰
        """

        logger.info("ğŸ”„ é‡æ–°è§„åˆ’ä»»åŠ¡...")

        # æ¸…ç©ºéƒ¨åˆ†å·¥ä½œè®°å¿†ï¼ˆç›¸å½“äº"å¿˜æ‰åˆšæ‰çš„å¤±è´¥å°è¯•"ï¼‰
        self.working_memory.memory_buffer.clear()

        # é‡æ–°åˆ†è§£ä»»åŠ¡
        replan_prompt = f"""
ä»»åŠ¡ï¼š{task}

åˆšæ‰çš„å°è¯•å¤±è´¥äº†ï¼Œè¯·é‡æ–°è§„åˆ’ï¼š

1. è¿™ä¸ªä»»åŠ¡çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
2. å¯èƒ½æœ‰å“ªäº›ä¸åŒçš„å®ç°è·¯å¾„ï¼Ÿï¼ˆåˆ—å‡º 2-3 ç§ï¼‰
3. æ¨èå“ªç§è·¯å¾„ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

ç»™å‡ºè¯¦ç»†çš„é‡æ–°è§„åˆ’æ–¹æ¡ˆã€‚
"""

        new_plan = await self.decision_maker.llm.generate(
            replan_prompt,
            max_tokens=2000
        )

        logger.info(f"ğŸ“‹ æ–°è®¡åˆ’:\n{new_plan}")

        # æ›´æ–°å­ç›®æ ‡
        # (ä» new_plan ä¸­æå–æ­¥éª¤ï¼Œæ›´æ–° working_memory.sub_goals)

    async def _save_experience(self, task: str, steps: int):
        """
        ä¿å­˜æˆåŠŸç»éªŒåˆ°é•¿æœŸè®°å¿†
        """

        experience = TaskExperience(
            task_description=task,
            action_sequence=self._extract_action_sequence(),
            total_steps=steps,
            success_rate=1.0
        )

        await self.long_term_memory.save_experience(experience)
        logger.info(f"ğŸ§  ç»éªŒå·²ä¿å­˜åˆ°é•¿æœŸè®°å¿†")
```

---

## å››ã€ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯”

### 4.1 ä¼ ç»Ÿæ–¹æ³•ï¼ˆé¢„å®šä¹‰è„šæœ¬ï¼‰

```python
# ä¼ ç»Ÿæ–¹æ³•ï¼šç¡¬ç¼–ç æ“ä½œåºåˆ—
def traditional_approach(task):
    # 1. é¢„å®šä¹‰æ“ä½œåºåˆ—
    actions = [
        {"type": "click", "target": "æ·˜å®å›¾æ ‡"},
        {"type": "click", "target": "æœç´¢æ¡†"},
        {"type": "input", "text": "åŒè‚©åŒ…"},
        {"type": "click", "target": "æœç´¢æŒ‰é’®"},
        # ... å›ºå®šæ­¥éª¤
    ]

    # 2. é¡ºåºæ‰§è¡Œ
    for action in actions:
        device.execute(action)
        time.sleep(1)

    # é—®é¢˜ï¼š
    # âŒ UI å˜åŒ–åå¤±æ•ˆ
    # âŒ æ— æ³•å¤„ç†å¼‚å¸¸ï¼ˆå¼¹çª—ã€åŠ è½½ï¼‰
    # âŒ æ— æ³•é€‚åº”ä¸åŒåœºæ™¯
    # âŒ ç¼ºä¹åé¦ˆå’Œçº é”™
```

### 4.2 äººç±»è®¤çŸ¥æ¨¡æ‹Ÿæ–¹æ³•ï¼ˆæœ¬æ–¹æ¡ˆï¼‰

```python
# äººç±»è®¤çŸ¥æ¨¡æ‹Ÿï¼šåŠ¨æ€è§‚å¯Ÿå†³ç­–
async def human_like_approach(task):
    agent = HumanLikeAgent(device)

    # æ ¸å¿ƒå¾ªç¯ï¼šè§‚å¯Ÿ â†’ æ€è€ƒ â†’ å†³ç­– â†’ æ‰§è¡Œ â†’ åé¦ˆ
    while not task_complete:
        # 1. è§‚å¯Ÿå½“å‰å±å¹•ï¼ˆè§†è§‰æ„ŸçŸ¥ï¼‰
        perception = await agent.vision.perceive_screen(screenshot)

        # 2. åŸºäºè§‚å¯Ÿ + è®°å¿†ï¼ŒåŠ¨æ€å†³ç­–
        decision = await agent.decision_maker.decide_next_action(
            perception=perception,
            goal=task
        )

        # 3. æ‰§è¡Œ + å³æ—¶åé¦ˆ
        result = await agent.trial_controller.execute_with_feedback(
            action=decision.action,
            expected_outcome=decision.expected_outcome
        )

        # 4. é”™è¯¯ç«‹å³çº æ­£
        if result.error:
            await agent._handle_error(result)

        # 5. æŒç»­è§‚å¯Ÿï¼ˆåå°ï¼‰
        # æ£€æµ‹å¼¹çª—ã€åŠ è½½ã€é”™è¯¯ç­‰

    # ä¼˜åŠ¿ï¼š
    # âœ… åŠ¨æ€é€‚åº” UI å˜åŒ–
    # âœ… è‡ªåŠ¨å¤„ç†å¼‚å¸¸
    # âœ… è¯•é”™å’Œçº æ­£
    # âœ… åŸºäºå®æ—¶è§‚å¯Ÿå†³ç­–
    # âœ… æœ‰çŸ­æœŸè®°å¿†ï¼Œé¿å…å¾ªç¯
```

---

## äº”ã€é«˜çº§ç‰¹æ€§

### 5.1 å¤šæ¨¡æ€æ„ŸçŸ¥èåˆ

```python
class MultiModalPerception:
    """
    å¤šæ¨¡æ€æ„ŸçŸ¥èåˆ
    æ•´åˆï¼šè§†è§‰ + æ–‡æœ¬ + ç©ºé—´ + æ—¶é—´
    """

    def __init__(self):
        self.vision_model = VLModel("Qwen2-VL-72B")
        self.ocr = PaddleOCR()
        self.ui_detector = UIDetector()
        self.spatial_memory = SpatialMemoryMap()  # ç©ºé—´è®°å¿†

    async def fuse_perception(self, screenshot: Image) -> FusedPerception:
        """
        èåˆå¤šæ¨¡æ€æ„ŸçŸ¥
        """

        # å¹¶è¡Œè·å–å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆä¸è€ƒè™‘ tokenï¼‰
        vision_result, ocr_result, ui_elements, spatial_context = await asyncio.gather(
            self.vision_model.analyze(screenshot, "è¯¦ç»†æè¿°è¿™ä¸ªå±å¹•"),
            self.ocr.extract_text(screenshot),
            self.ui_detector.detect(screenshot),
            self.spatial_memory.get_context(screenshot)
        )

        # èåˆï¼šè§†è§‰è¯­ä¹‰ + ç²¾ç¡®æ–‡å­— + å¯äº¤äº’å…ƒç´  + ç©ºé—´å…³ç³»
        fused = FusedPerception(
            semantic_understanding=vision_result,  # "è¿™æ˜¯ä¸€ä¸ªè´­ç‰© App çš„æœç´¢é¡µ"
            exact_text=ocr_result,                 # ["æœç´¢", "è´­ç‰©è½¦", "æˆ‘çš„"]
            interactive_elements=ui_elements,      # [Button(...), EditText(...)]
            spatial_layout=self._build_spatial_graph(ui_elements),  # å…ƒç´ ç©ºé—´å…³ç³»å›¾
            temporal_changes=self._compare_with_previous(screenshot)  # ä¸ä¸Šä¸€å¸§çš„å·®å¼‚
        )

        return fused

    def _build_spatial_graph(self, elements: List[UIElement]) -> SpatialGraph:
        """
        æ„å»ºç©ºé—´å…³ç³»å›¾
        ä¾‹å¦‚ï¼š"æœç´¢æ¡†"åœ¨"å¯¼èˆªæ "ä¸‹æ–¹ï¼Œ"è´­ç‰©è½¦å›¾æ ‡"åœ¨å³ä¸Šè§’
        """

        graph = SpatialGraph()

        for elem in elements:
            # æ·»åŠ èŠ‚ç‚¹
            graph.add_node(elem.id, elem)

            # æ·»åŠ ç©ºé—´å…³ç³»è¾¹
            for other in elements:
                if other.id == elem.id:
                    continue

                relation = self._compute_spatial_relation(elem, other)
                if relation:
                    graph.add_edge(elem.id, other.id, relation)

        return graph

    def _compute_spatial_relation(self, elem1, elem2) -> Optional[str]:
        """
        è®¡ç®—ä¸¤ä¸ªå…ƒç´ çš„ç©ºé—´å…³ç³»
        """

        # ä¸Šä¸‹å…³ç³»
        if elem1.bottom < elem2.top:
            return "above"
        elif elem1.top > elem2.bottom:
            return "below"

        # å·¦å³å…³ç³»
        if elem1.right < elem2.left:
            return "left_of"
        elif elem1.left > elem2.right:
            return "right_of"

        # åŒ…å«å…³ç³»
        if elem1.contains(elem2):
            return "contains"

        return None
```

### 5.2 ç©ºé—´è®°å¿†ç³»ç»Ÿ

```python
class SpatialMemoryMap:
    """
    ç©ºé—´è®°å¿†åœ°å›¾
    æ¨¡æ‹Ÿäººç±»çš„"è®°ä½æŸä¸ªæŒ‰é’®å¤§æ¦‚åœ¨å“ªä¸ªä½ç½®"

    ç±»ä¼¼ SLAMï¼ˆåŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼‰
    """

    def __init__(self):
        self.app_maps = {}  # {app_name: AppSpatialMap}

    def learn_layout(self, app_name: str, screen_type: str, elements: List[UIElement]):
        """
        å­¦ä¹  App çš„ç©ºé—´å¸ƒå±€
        """

        if app_name not in self.app_maps:
            self.app_maps[app_name] = AppSpatialMap(app_name)

        app_map = self.app_maps[app_name]

        # è®°å½•æ¯ä¸ªå…ƒç´ çš„"å…¸å‹ä½ç½®"
        for elem in elements:
            app_map.add_landmark(
                screen_type=screen_type,
                element_type=elem.type,
                element_text=elem.text,
                typical_position=elem.center,
                typical_size=elem.size
            )

    def recall_position(self, app_name: str, screen_type: str, target: str) -> Optional[Position]:
        """
        å›å¿†æŸä¸ªå…ƒç´ çš„å…¸å‹ä½ç½®
        """

        if app_name not in self.app_maps:
            return None

        app_map = self.app_maps[app_name]

        # æŸ¥è¯¢ç©ºé—´è®°å¿†
        landmark = app_map.get_landmark(screen_type, target)

        if landmark:
            return landmark.typical_position

        return None

    def predict_element_location(self, app_name: str, element_type: str, current_screen: Image):
        """
        é¢„æµ‹å…ƒç´ å¯èƒ½åœ¨å“ªé‡Œ
        åŸºäºï¼š
        1. å†å²ä½ç½®ç»Ÿè®¡
        2. UI è®¾è®¡æ¨¡å¼ï¼ˆå¦‚"è¿”å›æŒ‰é’®é€šå¸¸åœ¨å·¦ä¸Šè§’"ï¼‰
        3. å½“å‰å±å¹•çš„å¸ƒå±€ç‰¹å¾
        """

        # æ–¹æ³• 1: æŸ¥å†å²è®°å¿†
        historical = self.recall_position(app_name, "any", element_type)

        # æ–¹æ³• 2: åŸºäºè®¾è®¡æ¨¡å¼
        design_pattern = self._get_ui_design_pattern(element_type)

        # æ–¹æ³• 3: åŸºäºå½“å‰å±å¹•æ¨ç†
        current_layout = self._analyze_layout(current_screen)

        # èåˆä¸‰ç§ä¿¡æ¯ï¼Œç»™å‡ºæ¦‚ç‡åˆ†å¸ƒ
        probability_map = self._fuse_predictions(
            historical,
            design_pattern,
            current_layout
        )

        return probability_map

    def _get_ui_design_pattern(self, element_type: str) -> PositionPrior:
        """
        UI è®¾è®¡æ¨¡å¼å…ˆéªŒ
        """

        patterns = {
            "back_button": {"region": "top_left", "probability": 0.9},
            "search_button": {"region": "top_right", "probability": 0.8},
            "home_button": {"region": "bottom_center", "probability": 0.85},
            "settings": {"region": "top_right_or_bottom_right", "probability": 0.75},
        }

        return patterns.get(element_type, {"region": "anywhere", "probability": 0.1})
```

### 5.3 å…ƒè®¤çŸ¥ç›‘æ§ï¼ˆMetacognitionï¼‰

```python
class MetacognitiveMonitor:
    """
    å…ƒè®¤çŸ¥ç›‘æ§
    æ¨¡æ‹Ÿäººç±»çš„"è‡ªæˆ‘æ„è¯†"ï¼š"æˆ‘çŸ¥é“æˆ‘ä¸çŸ¥é“"

    åŠŸèƒ½ï¼š
    1. ç›‘æ§è‡ªå·±çš„ç†è§£ç¨‹åº¦
    2. è¯„ä¼°å†³ç­–è´¨é‡
    3. è§¦å‘"å¯»æ±‚å¸®åŠ©"æˆ–"æ›´ä»”ç»†æ€è€ƒ"
    """

    def __init__(self):
        self.confidence_threshold = 0.6

    async def monitor_understanding(self, perception: PerceptionResult) -> MetacognitiveState:
        """
        ç›‘æ§å¯¹å½“å‰å±å¹•çš„ç†è§£ç¨‹åº¦
        """

        # è¯„ä¼°æŒ‡æ ‡
        metrics = {
            "visual_clarity": self._assess_visual_clarity(perception),
            "semantic_understanding": perception.confidence,
            "actionability": len(perception.actionable_items) > 0,
            "consistency_with_memory": self._check_consistency(perception)
        }

        overall_confidence = np.mean(list(metrics.values()))

        if overall_confidence < self.confidence_threshold:
            # ç†è§£ä¸è¶³ï¼Œéœ€è¦æ›´ä»”ç»†è§‚å¯Ÿ
            return MetacognitiveState(
                status="uncertain",
                confidence=overall_confidence,
                recommendation="need_more_observation",
                reason=f"ç†è§£ç½®ä¿¡åº¦ä¸è¶³ ({overall_confidence:.0%})"
            )

        return MetacognitiveState(
            status="confident",
            confidence=overall_confidence,
            recommendation="proceed"
        )

    async def evaluate_decision(self, decision: Decision, perception: PerceptionResult) -> EvaluationResult:
        """
        è¯„ä¼°å†³ç­–è´¨é‡ï¼ˆåšä¹‹å‰"æƒ³ä¸€æƒ³é ä¸é è°±"ï¼‰
        """

        # ä½¿ç”¨å¦ä¸€ä¸ª LLM å®ä¾‹åš"ç¬¬äºŒæ„è§"
        evaluation_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ—è§‚è€…ï¼Œæ­£åœ¨è¯„ä¼°ä»¥ä¸‹å†³ç­–çš„åˆç†æ€§ï¼š

å½“å‰å±å¹•ï¼š{perception.screen_type}
å†³ç­–ï¼š{decision.description}
ç†ç”±ï¼š{decision.reasoning}
ç½®ä¿¡åº¦ï¼š{decision.confidence}

è¯·è¯„ä¼°ï¼š
1. è¿™ä¸ªå†³ç­–åˆç†å—ï¼Ÿ
2. æœ‰æ²¡æœ‰æ›´å¥½çš„é€‰æ‹©ï¼Ÿ
3. é£é™©å¦‚ä½•ï¼Ÿ
4. å»ºè®®æ‰§è¡Œè¿˜æ˜¯é‡æ–°è€ƒè™‘ï¼Ÿ

ç»™å‡ºè¯„åˆ†ï¼ˆ0-10ï¼‰å’Œå»ºè®®ã€‚
"""

        evaluation = await self.llm.generate(evaluation_prompt, max_tokens=500)

        score = self._extract_score(evaluation)

        if score < 6:
            return EvaluationResult(
                approved=False,
                score=score,
                suggestion="reconsider",
                reason=evaluation
            )

        return EvaluationResult(
            approved=True,
            score=score,
            suggestion="proceed"
        )
```

---

## å…­ã€å®æ–½å»ºè®®

### 6.1 æ¸è¿›å¼å®ç°è·¯çº¿

#### Phase 1: åŸºç¡€è®¤çŸ¥å¾ªç¯ï¼ˆ1-2 å‘¨ï¼‰

```python
# æœ€å°å¯è¡Œå®ç°
- [x] è§†è§‰æ„ŸçŸ¥ï¼ˆQwen2-VL å¤šæ¨¡æ€ç†è§£ï¼‰
- [x] å·¥ä½œè®°å¿†ï¼ˆdeque + ä¸Šä¸‹æ–‡å¿«ç…§ï¼‰
- [x] åŸºç¡€å†³ç­–ï¼ˆChain of Thought æ¨ç†ï¼‰
- [x] è¯•é”™æ§åˆ¶ï¼ˆæ‰§è¡Œ + å³æ—¶åˆ¤æ–­ï¼‰
- [x] ä¸»å¾ªç¯ï¼ˆè§‚å¯Ÿ â†’ å†³ç­– â†’ æ‰§è¡Œ â†’ åé¦ˆï¼‰
```

#### Phase 2: å¢å¼ºç‰¹æ€§ï¼ˆ2-3 å‘¨ï¼‰

```python
- [ ] æŒç»­è§‚å¯Ÿï¼ˆåå°äº‹ä»¶æ£€æµ‹ï¼‰
- [ ] å¾ªç¯/å¡ä½æ£€æµ‹
- [ ] å¼¹çª—/åŠ è½½è‡ªåŠ¨å¤„ç†
- [ ] å¤±è´¥æ¨¡å¼å­¦ä¹ 
- [ ] äºŒæ¬¡éªŒè¯æœºåˆ¶
```

#### Phase 3: é«˜çº§è®¤çŸ¥ï¼ˆ3-4 å‘¨ï¼‰

```python
- [ ] ç©ºé—´è®°å¿†åœ°å›¾
- [ ] å¤šæ¨¡æ€æ„ŸçŸ¥èåˆ
- [ ] å…ƒè®¤çŸ¥ç›‘æ§
- [ ] ä¸é•¿æœŸè®°å¿†é›†æˆ
- [ ] ç»éªŒæ³›åŒ–
```

### 6.2 æœ¬åœ°æ¨¡å‹æ¨è

```yaml
# è§†è§‰ç†è§£ï¼ˆå¿…éœ€ï¼‰
Vision-Language Model:
  - Qwen2-VL-72B-Instruct (æ¨è)
  - InternVL2-76B
  - LLaVA-Next-34B

# è¯­è¨€æ¨ç†ï¼ˆå¿…éœ€ï¼‰
LLM:
  - Qwen2.5-72B-Instruct (æ¨è)
  - Llama-3.1-70B-Instruct
  - DeepSeek-V2.5

# UI å…ƒç´ æ£€æµ‹ï¼ˆå¯é€‰ï¼‰
Object Detection:
  - OWLv2 (å¼€æ”¾è¯æ±‡æ£€æµ‹)
  - Grounding DINO

# OCRï¼ˆæ¨èï¼‰
Text Recognition:
  - PaddleOCR
  - TrOCR
```

### 6.3 ç¡¬ä»¶éœ€æ±‚

```
æœ€ä½é…ç½®ï¼š
- GPU: RTX 4090 (24GB) Ã— 1
- RAM: 64GB
- å­˜å‚¨: 500GB SSD

æ¨èé…ç½®ï¼š
- GPU: RTX 4090 (24GB) Ã— 2 æˆ– A100 (80GB) Ã— 1
- RAM: 128GB
- å­˜å‚¨: 1TB NVMe SSD

è¯´æ˜ï¼š
- 72B æ¨¡å‹é‡åŒ–åˆ° 4-bit çº¦å ç”¨ 40GB æ˜¾å­˜
- å¯åŒæ—¶è¿è¡Œ VL æ¨¡å‹ + LLM
- ä¸è€ƒè™‘ token æ¶ˆè€—ï¼Œä¸“æ³¨æ•ˆæœ
```

---

## ä¸ƒã€è°ƒè¯•ä¸å¯è§†åŒ–

### 7.1 è®¤çŸ¥è¿‡ç¨‹å¯è§†åŒ–

```python
class CognitiveVisualizer:
    """
    è®¤çŸ¥è¿‡ç¨‹å¯è§†åŒ–
    å¸®åŠ©ç†è§£ Agent åœ¨"æƒ³ä»€ä¹ˆ"
    """

    def visualize_perception(self, screenshot, perception):
        """
        å¯è§†åŒ–æ„ŸçŸ¥ç»“æœ
        """

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. åŸå§‹æˆªå›¾ + æ³¨æ„åŠ›çƒ­åŠ›å›¾
        axes[0, 0].imshow(screenshot)
        axes[0, 0].imshow(perception.attention_focus.heatmap, alpha=0.5, cmap='jet')
        axes[0, 0].set_title("Attention Map")

        # 2. UI å…ƒç´ æ ‡æ³¨
        axes[0, 1].imshow(screenshot)
        for elem in perception.actionable_items:
            rect = plt.Rectangle(
                (elem.x, elem.y),
                elem.width,
                elem.height,
                fill=False,
                color='green',
                linewidth=2
            )
            axes[0, 1].add_patch(rect)
            axes[0, 1].text(elem.x, elem.y - 5, elem.text, color='green')
        axes[0, 1].set_title("UI Elements")

        # 3. OCR æ–‡å­—
        axes[1, 0].imshow(screenshot)
        axes[1, 0].text(10, 10, perception.text_content, color='yellow')
        axes[1, 0].set_title("OCR Text")

        # 4. è¯­ä¹‰ç†è§£
        axes[1, 1].axis('off')
        axes[1, 1].text(0.1, 0.5, perception.current_context, fontsize=10)
        axes[1, 1].set_title("Semantic Understanding")

        plt.tight_layout()
        plt.savefig(f"perception_{time.time()}.png")

    def visualize_working_memory(self, memory: WorkingMemory):
        """
        å¯è§†åŒ–å·¥ä½œè®°å¿†
        """

        print("\n" + "="*60)
        print("ğŸ§  å·¥ä½œè®°å¿†çŠ¶æ€")
        print("="*60)

        print(f"\nğŸ¯ ä¸»ç›®æ ‡: {memory.current_goal}")
        print(f"\nğŸ“‹ å­ç›®æ ‡æ ˆ: {' â†’ '.join(memory.sub_goals)}")

        print("\nğŸ“ æœ€è¿‘æ“ä½œ:")
        for i, action in enumerate(memory.get_recent_actions(5)):
            print(f"  {i+1}. {action}")

        print("\nğŸ’¡ é‡è¦è®°å¿†:")
        important = [m for m in memory.memory_buffer if m.importance > 0.7]
        for m in important[-3:]:
            print(f"  - {m.content} (é‡è¦åº¦: {m.importance:.0%})")

        print("="*60 + "\n")

    def visualize_decision_process(self, thinking: str, decision: Decision):
        """
        å¯è§†åŒ–å†³ç­–è¿‡ç¨‹
        """

        print("\n" + "="*60)
        print("ğŸ’­ å†³ç­–æ€è€ƒè¿‡ç¨‹")
        print("="*60)
        print(thinking)
        print("\n" + "-"*60)
        print(f"âœ… æœ€ç»ˆå†³ç­–: {decision.description}")
        print(f"   ç½®ä¿¡åº¦: {decision.confidence:.0%}")
        print(f"   é¢„æœŸç»“æœ: {decision.expected_outcome}")
        print("="*60 + "\n")
```

### 7.2 äº¤äº’å¼è°ƒè¯•

```python
class InteractiveDebugger:
    """
    äº¤äº’å¼è°ƒè¯•å™¨
    å…è®¸äººç±»ä»‹å…¥ Agent çš„å†³ç­–è¿‡ç¨‹
    """

    def __init__(self, agent: HumanLikeAgent):
        self.agent = agent
        self.pause_on_decision = False
        self.step_by_step = True

    async def run_with_debug(self, task: str):
        """
        å¸¦è°ƒè¯•çš„è¿è¡Œ
        """

        for step in range(50):
            # æ„ŸçŸ¥
            perception = await self.agent.vision.perceive_screen(screenshot)

            # å†³ç­–
            decision = await self.agent.decision_maker.decide_next_action(
                perception, task
            )

            # === æš‚åœè®©äººç±»æŸ¥çœ‹ ===
            if self.step_by_step:
                print(f"\nâ¸ï¸  æ­¥éª¤ {step + 1} - æš‚åœ")
                print(f"   å†³ç­–: {decision.description}")
                print(f"   ç½®ä¿¡åº¦: {decision.confidence:.0%}")

                choice = input("\né€‰æ‹©: (c)ontinue / (s)kip / (m)odify / (q)uit: ")

                if choice == 'q':
                    break
                elif choice == 's':
                    continue
                elif choice == 'm':
                    # å…è®¸äººç±»ä¿®æ”¹å†³ç­–
                    new_action = input("è¾“å…¥æ–°çš„æ“ä½œæè¿°: ")
                    decision.description = new_action

            # æ‰§è¡Œ
            result = await self.agent.trial_controller.execute_with_feedback(
                decision.action,
                self.agent.device,
                decision.expected_outcome
            )

            print(f"   ç»“æœ: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")

            if not result.success:
                print(f"   åŸå› : {result.reason}")
                if input("ç»§ç»­ï¼Ÿ(y/n): ") == 'n':
                    break
```

---

## å…«ã€æ€»ç»“

### æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **æŒç»­è§‚å¯Ÿå¾ªç¯** - ä¸æ˜¯"æ‰§è¡Œå®Œå°±ä¸ç®¡"ï¼Œè€Œæ˜¯æŒç»­ç›‘æ§å±å¹•å˜åŒ–
2. **å·¥ä½œè®°å¿†ç³»ç»Ÿ** - æ¨¡æ‹Ÿäººç±»çš„çŸ­æœŸè®°å¿†ï¼Œé¿å…åŸåœ°æ‰“è½¬
3. **è¯•é”™çº é”™æœºåˆ¶** - æ‰§è¡Œåç«‹å³åˆ¤æ–­ï¼Œé”™è¯¯ç«‹å³å›é€€
4. **åŠ¨æ€å†³ç­–** - ä¸é¢„å®šä¹‰è„šæœ¬ï¼ŒåŸºäºå®æ—¶è§‚å¯Ÿçµæ´»å†³ç­–
5. **è®¤çŸ¥æ€ç»´é“¾** - ä¸ç›´æ¥è¾“å‡ºæ“ä½œï¼Œè€Œæ˜¯å…ˆæ·±åº¦æ€è€ƒå†å†³ç­–

### ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”

| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | äººç±»è®¤çŸ¥æ¨¡æ‹Ÿ |
|------|---------|-------------|
| æ“ä½œæ–¹å¼ | é¢„å®šä¹‰è„šæœ¬ | åŠ¨æ€è§‚å¯Ÿå†³ç­– |
| é€‚åº”æ€§ | UI å˜åŒ–å³å¤±æ•ˆ | è‡ªé€‚åº” UI å˜åŒ– |
| é”™è¯¯å¤„ç† | æ— æˆ–è¢«åŠ¨ | ä¸»åŠ¨æ£€æµ‹å’Œçº æ­£ |
| è®°å¿† | æ— çŠ¶æ€ | å·¥ä½œè®°å¿† + é•¿æœŸè®°å¿† |
| æ€è€ƒè¿‡ç¨‹ | æ—  | å®Œæ•´ CoT æ¨ç† |
| å¼‚å¸¸å¤„ç† | è„†å¼± | è‡ªåŠ¨å¤„ç†å¼¹çª—/åŠ è½½/é”™è¯¯ |
| Token æ¶ˆè€— | ä½ | é«˜ï¼ˆæœ¬åœ°æ¨¡å‹æ— é™åˆ¶ï¼‰ |
| æˆåŠŸç‡ | 60-70% | 90%+ |

### é€‚ç”¨åœºæ™¯

âœ… **æœ€é€‚åˆ**ï¼š
- å¤æ‚å¤šæ­¥ä»»åŠ¡ï¼ˆå¦‚"åœ¨ç”µå•† App å®Œæˆè´­ç‰©æµç¨‹"ï¼‰
- UI é¢‘ç¹å˜åŒ–çš„ App
- éœ€è¦å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µï¼ˆå¼¹çª—ã€åŠ è½½ã€é”™è¯¯ï¼‰
- æ¢ç´¢æ€§ä»»åŠ¡ï¼ˆç¬¬ä¸€æ¬¡æ¥è§¦æŸä¸ª Appï¼‰

âŒ **ä¸é€‚åˆ**ï¼š
- ç®€å•å•æ­¥æ“ä½œï¼ˆå¦‚"ç‚¹å‡»æŸä¸ªå›ºå®šæŒ‰é’®"ï¼‰
- å¯¹å»¶è¿Ÿæ•æ„Ÿçš„å®æ—¶æ“ä½œ
- èµ„æºå—é™ç¯å¢ƒï¼ˆéœ€è¦å¤§æ¨¡å‹æ”¯æŒï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025-10-31
**ç»´æŠ¤è€…**: DroidRun-VL Team
