"""
æµ‹è¯• vLLM æœåŠ¡ç¨³å®šæ€§
æ¯æ¬¡éƒ½æ–°å¯ä¼šè¯,ä¸åœè¯¢é—®é—®é¢˜,æ— é™å¾ªç¯
"""
import os
import time
import random
from datetime import datetime
from typing import List, Optional

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = "sk-Kd92LE2pud8bVtZE23B47248Bc064006Af400cB6770c8577"

from droidrun.agent.utils.llm_picker import load_llm

# æµ‹è¯•é—®é¢˜åˆ—è¡¨ - æ›´å¤æ‚çš„é—®é¢˜
TEST_QUESTIONS: List[str] = [
    "è¯·è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ ä¸­çš„åå‘ä¼ æ’­ç®—æ³•,åŒ…æ‹¬é“¾å¼æ³•åˆ™çš„åº”ç”¨ã€æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜,ä»¥åŠå¦‚ä½•é€šè¿‡æ‰¹å½’ä¸€åŒ–ã€æ®‹å·®è¿æ¥ç­‰æŠ€æœ¯æ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚è¯·ç”¨æ•°å­¦å…¬å¼å’Œä»£ç ç¤ºä¾‹è¯´æ˜ã€‚",
    "è®¾è®¡ä¸€ä¸ªé«˜å¹¶å‘çš„åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿ,éœ€è¦è€ƒè™‘ç¼“å­˜ä¸€è‡´æ€§ã€ç¼“å­˜ç©¿é€ã€ç¼“å­˜é›ªå´©ã€ç¼“å­˜å‡»ç©¿ç­‰é—®é¢˜ã€‚è¯·è¯¦ç»†è¯´æ˜æ¶æ„è®¾è®¡ã€æ•°æ®åˆ†ç‰‡ç­–ç•¥ã€ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•çš„å®ç°ã€ä»¥åŠå¦‚ä½•ä¿è¯é«˜å¯ç”¨æ€§ã€‚",
    "è¯·å®ç°ä¸€ä¸ªå®Œæ•´çš„äºŒå‰æœç´¢æ ‘,åŒ…æ‹¬æ’å…¥ã€åˆ é™¤ã€æŸ¥æ‰¾ã€ä¸­åºéå†ã€å‰åºéå†ã€ååºéå†ã€å±‚åºéå†ã€æŸ¥æ‰¾æœ€å°å€¼ã€æŸ¥æ‰¾æœ€å¤§å€¼ã€æŸ¥æ‰¾å‰é©±èŠ‚ç‚¹ã€æŸ¥æ‰¾åç»§èŠ‚ç‚¹ç­‰æ“ä½œã€‚è¦æ±‚ä»£ç æœ‰å®Œæ•´çš„æ³¨é‡Šå’Œæ—¶é—´å¤æ‚åº¦åˆ†æã€‚",
    "è§£é‡Šç°ä»£æ“ä½œç³»ç»Ÿä¸­çš„å†…å­˜ç®¡ç†æœºåˆ¶,åŒ…æ‹¬è™šæ‹Ÿå†…å­˜ã€åˆ†é¡µã€åˆ†æ®µã€é¡µé¢ç½®æ¢ç®—æ³•(FIFOã€LRUã€LFUã€Clock)ã€å†…å­˜æ˜ å°„æ–‡ä»¶ã€å†™æ—¶å¤åˆ¶(Copy-on-Write)ç­‰ã€‚å¹¶è¯´æ˜Linuxå†…æ ¸æ˜¯å¦‚ä½•å®ç°è¿™äº›æœºåˆ¶çš„ã€‚",
    "è¯·è®¾è®¡å¹¶å®ç°ä¸€ä¸ªåˆ†å¸ƒå¼äº‹åŠ¡è§£å†³æ–¹æ¡ˆ,å¯¹æ¯”2PCã€3PCã€TCCã€Sagaç­‰ä¸åŒæ¨¡å¼çš„ä¼˜ç¼ºç‚¹ã€‚è¯¦ç»†è¯´æ˜åœ¨å¾®æœåŠ¡æ¶æ„ä¸‹å¦‚ä½•ä¿è¯æ•°æ®ä¸€è‡´æ€§,ä»¥åŠå¦‚ä½•å¤„ç†ç½‘ç»œåˆ†åŒºã€èŠ‚ç‚¹æ•…éšœç­‰å¼‚å¸¸æƒ…å†µã€‚",
    "è¯¦ç»†è§£é‡ŠTCPåè®®çš„æ‹¥å¡æ§åˆ¶æœºåˆ¶,åŒ…æ‹¬æ…¢å¯åŠ¨ã€æ‹¥å¡é¿å…ã€å¿«é€Ÿé‡ä¼ ã€å¿«é€Ÿæ¢å¤ç®—æ³•ã€‚è¯´æ˜TCPå¦‚ä½•é€šè¿‡æ»‘åŠ¨çª—å£ã€ç´¯ç§¯ç¡®è®¤ã€é€‰æ‹©æ€§ç¡®è®¤(SACK)æ¥æé«˜ä¼ è¾“æ•ˆç‡ã€‚å¹¶å¯¹æ¯”TCP Renoã€TCP Vegasã€TCP BBRç­‰ä¸åŒå˜ç§ã€‚",
    "è¯·å®ç°ä¸€ä¸ªæ”¯æŒäº‹åŠ¡çš„é”®å€¼å­˜å‚¨å¼•æ“,éœ€è¦å®ç°MVCC(å¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶)ã€WAL(é¢„å†™æ—¥å¿—)ã€LSMæ ‘æˆ–B+æ ‘å­˜å‚¨ç»“æ„ã€Compactionå‹ç¼©ç­–ç•¥ã€å´©æºƒæ¢å¤æœºåˆ¶ã€‚è¦æ±‚ä»£ç ç»“æ„æ¸…æ™°,å¹¶è¯´æ˜æ¯ä¸ªæ¨¡å—çš„è®¾è®¡æ€è·¯ã€‚",
    "æ·±å…¥è§£é‡ŠJavaè™šæ‹Ÿæœºçš„åƒåœ¾å›æ”¶æœºåˆ¶,åŒ…æ‹¬æ ‡è®°-æ¸…é™¤ã€æ ‡è®°-æ•´ç†ã€å¤åˆ¶ç®—æ³•ã€åˆ†ä»£å›æ”¶ç­–ç•¥ã€‚è¯¦ç»†è¯´æ˜Serialã€ParNewã€Parallel Scavengeã€CMSã€G1ã€ZGCç­‰ä¸åŒåƒåœ¾å›æ”¶å™¨çš„å·¥ä½œåŸç†ã€é€‚ç”¨åœºæ™¯å’Œè°ƒä¼˜å‚æ•°ã€‚",
    "è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„å…¨æ–‡æœç´¢å¼•æ“,éœ€è¦å®ç°å€’æ’ç´¢å¼•ã€TF-IDFç®—æ³•ã€BM25ç®—æ³•ã€å‘é‡ç©ºé—´æ¨¡å‹ã€å¸ƒå°”æ£€ç´¢ã€çŸ­è¯­æŸ¥è¯¢ã€æ¨¡ç³ŠæŸ¥è¯¢ã€æ‹¼å†™çº é”™ã€æŸ¥è¯¢å»ºè®®ç­‰åŠŸèƒ½ã€‚è¯´æ˜å¦‚ä½•ä¼˜åŒ–ç´¢å¼•æ„å»ºå’ŒæŸ¥è¯¢æ€§èƒ½ã€‚",
    "è¯·è¯¦ç»†è§£é‡ŠRaftå…±è¯†ç®—æ³•çš„å·¥ä½œåŸç†,åŒ…æ‹¬é¢†å¯¼è€…é€‰ä¸¾ã€æ—¥å¿—å¤åˆ¶ã€å®‰å…¨æ€§ä¿è¯ã€é›†ç¾¤æˆå‘˜å˜æ›´ã€æ—¥å¿—å‹ç¼©ç­‰æœºåˆ¶ã€‚å¯¹æ¯”Paxosç®—æ³•çš„å¼‚åŒ,å¹¶è¯´æ˜åœ¨å®é™…å·¥ç¨‹ä¸­å¦‚ä½•å®ç°ä¸€ä¸ªé«˜å¯ç”¨çš„Rafté›†ç¾¤ã€‚",
    "å®ç°ä¸€ä¸ªæ”¯æŒå¤šç§è·¯ç”±ç­–ç•¥çš„APIç½‘å…³,åŒ…æ‹¬è´Ÿè½½å‡è¡¡(è½®è¯¢ã€éšæœºã€æœ€å°è¿æ¥æ•°ã€ä¸€è‡´æ€§å“ˆå¸Œ)ã€ç†”æ–­ã€é™æµã€é‡è¯•ã€è¶…æ—¶æ§åˆ¶ã€æœåŠ¡å‘ç°ã€å¥åº·æ£€æŸ¥ã€åŠ¨æ€é…ç½®æ›´æ–°ç­‰åŠŸèƒ½ã€‚è¦æ±‚æ”¯æŒçƒ­æ›´æ–°è€Œä¸å½±å“ç°æœ‰è¿æ¥ã€‚",
    "æ·±å…¥è§£é‡Šç°ä»£ç¼–è¯‘å™¨çš„å·¥ä½œåŸç†,åŒ…æ‹¬è¯æ³•åˆ†æã€è¯­æ³•åˆ†æã€è¯­ä¹‰åˆ†æã€ä¸­é—´ä»£ç ç”Ÿæˆã€ä»£ç ä¼˜åŒ–(å¸¸é‡æŠ˜å ã€æ­»ä»£ç æ¶ˆé™¤ã€å¾ªç¯ä¼˜åŒ–ã€å†…è”ç­‰)ã€ç›®æ ‡ä»£ç ç”Ÿæˆã€‚å¹¶å®ç°ä¸€ä¸ªç®€å•çš„è¡¨è¾¾å¼è§£æå™¨å’Œæ±‚å€¼å™¨ã€‚",
    "è®¾è®¡ä¸€ä¸ªåˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ,éœ€è¦æ”¯æŒå®šæ—¶ä»»åŠ¡ã€ä¾èµ–ä»»åŠ¡ã€å¤±è´¥é‡è¯•ã€ä»»åŠ¡ä¼˜å…ˆçº§ã€èµ„æºéš”ç¦»ã€ä»»åŠ¡ç›‘æ§ã€åŠ¨æ€æ‰©ç¼©å®¹ç­‰åŠŸèƒ½ã€‚è¯´æ˜å¦‚ä½•ä¿è¯ä»»åŠ¡ä¸è¢«é‡å¤æ‰§è¡Œ,ä»¥åŠå¦‚ä½•å¤„ç†è°ƒåº¦å™¨èŠ‚ç‚¹æ•…éšœã€‚",
    "è¯·è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformeræ¶æ„,åŒ…æ‹¬è‡ªæ³¨æ„åŠ›ã€å¤šå¤´æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç ã€å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥ã€å±‚å½’ä¸€åŒ–ç­‰ã€‚è¯´æ˜BERTã€GPTã€T5ç­‰é¢„è®­ç»ƒæ¨¡å‹çš„åŒºåˆ«,ä»¥åŠå¦‚ä½•è¿›è¡Œå¾®è°ƒå’Œæ¨ç†ä¼˜åŒ–ã€‚",
    "å®ç°ä¸€ä¸ªé«˜æ€§èƒ½çš„æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ,éœ€è¦æ”¯æŒæŒä¹…åŒ–ã€æ¶ˆæ¯ç¡®è®¤ã€æ¶ˆæ¯é‡è¯•ã€æ­»ä¿¡é˜Ÿåˆ—ã€å»¶è¿Ÿé˜Ÿåˆ—ã€æ¶ˆæ¯è¿‡æ»¤ã€æ¶ˆæ¯è¿½è¸ªã€äº‹åŠ¡æ¶ˆæ¯ç­‰åŠŸèƒ½ã€‚è¯´æ˜å¦‚ä½•ä¿è¯æ¶ˆæ¯ä¸ä¸¢å¤±ã€ä¸é‡å¤ã€é¡ºåºæ€§,ä»¥åŠå¦‚ä½•ä¼˜åŒ–ååé‡å’Œå»¶è¿Ÿã€‚",
    "æ·±å…¥è§£é‡Šæ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨çš„å·¥ä½œåŸç†,åŒ…æ‹¬æŸ¥è¯¢é‡å†™ã€ç´¢å¼•é€‰æ‹©ã€è¿æ¥ç®—æ³•(åµŒå¥—å¾ªç¯ã€å“ˆå¸Œè¿æ¥ã€å½’å¹¶è¿æ¥)ã€ä»£ä»·ä¼°ç®—ã€ç»Ÿè®¡ä¿¡æ¯ã€æ‰§è¡Œè®¡åˆ’ç”Ÿæˆç­‰ã€‚å¹¶è¯´æ˜å¦‚ä½•åˆ†æå’Œä¼˜åŒ–æ…¢æŸ¥è¯¢ã€‚",
    "è®¾è®¡ä¸€ä¸ªæ”¯æŒå®æ—¶æ•°æ®å¤„ç†çš„æµå¼è®¡ç®—æ¡†æ¶,éœ€è¦å®ç°çª—å£æ“ä½œ(æ»šåŠ¨çª—å£ã€æ»‘åŠ¨çª—å£ã€ä¼šè¯çª—å£)ã€æ°´ä½çº¿æœºåˆ¶ã€çŠ¶æ€ç®¡ç†ã€Exactly-Onceè¯­ä¹‰ã€åå‹æœºåˆ¶ã€å®¹é”™æ¢å¤ç­‰åŠŸèƒ½ã€‚å¯¹æ¯”Flinkã€Spark Streamingã€Stormçš„ä¼˜ç¼ºç‚¹ã€‚",
    "è¯·å®ç°ä¸€ä¸ªæ”¯æŒåŠ¨æ€ä»£ç†å’ŒAOPçš„IoCå®¹å™¨,éœ€è¦æ”¯æŒæ„é€ å™¨æ³¨å…¥ã€å±æ€§æ³¨å…¥ã€æ–¹æ³•æ³¨å…¥ã€å¾ªç¯ä¾èµ–æ£€æµ‹ã€Beanç”Ÿå‘½å‘¨æœŸç®¡ç†ã€ä½œç”¨åŸŸç®¡ç†(å•ä¾‹ã€åŸå‹)ã€æ¡ä»¶è£…é…ã€é…ç½®å±æ€§ç»‘å®šç­‰åŠŸèƒ½ã€‚è¯´æ˜Springçš„å®ç°åŸç†ã€‚",
    "æ·±å…¥è§£é‡Šå¯†ç å­¦ä¸­çš„éå¯¹ç§°åŠ å¯†ç®—æ³•,åŒ…æ‹¬RSAã€ECCã€ElGamalã€DSAç­‰ã€‚è¯¦ç»†è¯´æ˜å¯†é’¥ç”Ÿæˆã€åŠ å¯†ã€è§£å¯†ã€æ•°å­—ç­¾åã€å¯†é’¥äº¤æ¢çš„æ•°å­¦åŸç†ã€‚å¹¶è¯´æ˜å¦‚ä½•åœ¨TLS/SSLä¸­åº”ç”¨è¿™äº›ç®—æ³•,ä»¥åŠé‡å­è®¡ç®—å¯¹å¯†ç å­¦çš„å¨èƒã€‚",
    "è®¾è®¡ä¸€ä¸ªæ”¯æŒå¤šç§Ÿæˆ·çš„äº‘åŸç”Ÿåº”ç”¨å¹³å°,éœ€è¦å®ç°èµ„æºéš”ç¦»ã€é…é¢ç®¡ç†ã€å¼¹æ€§ä¼¸ç¼©ã€æœåŠ¡ç½‘æ ¼ã€å¯è§‚æµ‹æ€§(æ—¥å¿—ã€æŒ‡æ ‡ã€è¿½è¸ª)ã€æŒç»­éƒ¨ç½²ã€ç°åº¦å‘å¸ƒã€æ•…éšœæ³¨å…¥æµ‹è¯•ç­‰åŠŸèƒ½ã€‚è¯´æ˜å¦‚ä½•åŸºäºKubernetesæ„å»ºè¿™æ ·çš„å¹³å°ã€‚",
]


def calculate_token_speed(
    response_length: int,
    time_taken: float,
    avg_chars_per_token: float = 2.5
) -> tuple[int, float]:
    """
    è®¡ç®— token é€Ÿåº¦

    Args:
        response_length: å“åº”å­—ç¬¦æ•°
        time_taken: è€—æ—¶(ç§’)
        avg_chars_per_token: å¹³å‡æ¯ä¸ªtokençš„å­—ç¬¦æ•°(ä¸­æ–‡çº¦2-3,è‹±æ–‡çº¦4)

    Returns:
        (ä¼°è®¡çš„tokenæ•°, tokens/ç§’)
    """
    estimated_tokens = int(response_length / avg_chars_per_token)
    tokens_per_second = estimated_tokens / time_taken if time_taken > 0 else 0
    return estimated_tokens, tokens_per_second


def test_single_session(
    session_id: int,
    api_base: str,
    model: str,
    timeout: float = 120.0
) -> dict:
    """
    æµ‹è¯•å•ä¸ªä¼šè¯

    Args:
        session_id: ä¼šè¯ç¼–å·
        api_base: API åŸºç¡€ URL
        model: æ¨¡å‹åç§°
        timeout: è¶…æ—¶æ—¶é—´(ç§’)

    Returns:
        åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
    """
    result = {
        "session_id": session_id,
        "timestamp": datetime.now().isoformat(),
        "question": None,
        "success": False,
        "response_length": 0,
        "time_taken": 0.0,
        "estimated_tokens": 0,
        "tokens_per_second": 0.0,
        "error": None,
    }

    # éšæœºé€‰æ‹©ä¸€ä¸ªé—®é¢˜
    question = random.choice(TEST_QUESTIONS)
    result["question"] = question

    start_time = time.time()

    try:
        # æ¯æ¬¡åˆ›å»ºæ–°çš„ LLM å®ä¾‹ (æ–°ä¼šè¯)
        llm = load_llm(
            provider_name="OpenAILike",
            model=model,
            api_base=api_base,
            api_key=os.environ["OPENAI_API_KEY"],
            temperature=0.7,
            request_timeout=timeout,
        )

        # è°ƒç”¨ LLM
        response = llm.complete(question, timeout=timeout)

        result["success"] = True
        result["response_length"] = len(response.text)
        result["time_taken"] = time.time() - start_time

        # è®¡ç®— token é€Ÿåº¦
        estimated_tokens, tokens_per_second = calculate_token_speed(
            result["response_length"],
            result["time_taken"]
        )
        result["estimated_tokens"] = estimated_tokens
        result["tokens_per_second"] = tokens_per_second

        print(f"âœ… ä¼šè¯ {session_id}: æˆåŠŸ")
        print(f"   â±ï¸  è€—æ—¶: {result['time_taken']:.2f}s")
        print(f"   ğŸ“ å“åº”: {result['response_length']} å­—ç¬¦ / ~{estimated_tokens} tokens")
        print(f"   ğŸš€ é€Ÿåº¦: {tokens_per_second:.1f} tokens/s")
        print(f"   ğŸ’¬ é—®é¢˜: {question[:80]}...")
        print(f"   ğŸ“„ å“åº”é¢„è§ˆ: {response.text[:150]}...")

    except Exception as e:
        result["success"] = False
        result["error"] = str(e)
        result["time_taken"] = time.time() - start_time

        print(f"âŒ ä¼šè¯ {session_id}: å¤±è´¥ ({result['time_taken']:.2f}s)")
        print(f"   ğŸ’¬ é—®é¢˜: {question[:80]}...")
        print(f"   âš ï¸  é”™è¯¯: {e}")

    return result


def run_infinite_stability_test(
    api_base: str,
    model: str = "/models",
    delay_between_requests: float = 1.0,
    timeout: float = 120.0,
    save_log_every: int = 10,
):
    """
    è¿è¡Œæ— é™å¾ªç¯çš„ç¨³å®šæ€§æµ‹è¯•

    Args:
        api_base: API åŸºç¡€ URL
        model: æ¨¡å‹åç§°
        delay_between_requests: è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿ(ç§’)
        timeout: å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
        save_log_every: æ¯Næ¬¡æµ‹è¯•ä¿å­˜ä¸€æ¬¡æ—¥å¿—
    """
    print("=" * 100)
    print("ğŸš€ vLLM æ— é™å¾ªç¯ç¨³å®šæ€§æµ‹è¯•")
    print("=" * 100)
    print(f"API Base: {api_base}")
    print(f"Model: {model}")
    print(f"è¯·æ±‚é—´éš”: {delay_between_requests}s")
    print(f"è¶…æ—¶æ—¶é—´: {timeout}s")
    print(f"æ¯ {save_log_every} æ¬¡ä¿å­˜æ—¥å¿—")
    print("æŒ‰ Ctrl+C åœæ­¢æµ‹è¯•")
    print("=" * 100)
    print()

    results = []
    session_id = 0
    success_count = 0
    total_time = 0.0
    total_tokens = 0
    total_chars = 0

    test_start_time = time.time()
    last_save_time = test_start_time

    try:
        while True:
            session_id += 1
            print(f"\n{'='*100}")
            print(f"ğŸ”„ å¼€å§‹ä¼šè¯ #{session_id} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"{'='*100}")

            result = test_single_session(session_id, api_base, model, timeout)
            results.append(result)

            if result["success"]:
                success_count += 1
                total_time += result["time_taken"]
                total_tokens += result["estimated_tokens"]
                total_chars += result["response_length"]

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            current_time = time.time()
            elapsed_time = current_time - test_start_time
            success_rate = (success_count / session_id) * 100

            print(f"\n{'â”€'*100}")
            print(f"ğŸ“Š ç´¯è®¡ç»Ÿè®¡:")
            print(f"   æ€»è¯·æ±‚: {session_id} | æˆåŠŸ: {success_count} | å¤±è´¥: {session_id - success_count} | æˆåŠŸç‡: {success_rate:.1f}%")

            if success_count > 0:
                avg_response_time = total_time / success_count
                avg_tokens = total_tokens / success_count
                avg_token_speed = total_tokens / total_time if total_time > 0 else 0

                print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s")
                print(f"   å¹³å‡ tokens: {avg_tokens:.0f} tokens/è¯·æ±‚")
                print(f"   å¹³å‡é€Ÿåº¦: {avg_token_speed:.1f} tokens/s")
                print(f"   æ€» tokens: {total_tokens} (~{total_chars} å­—ç¬¦)")

            print(f"   è¿è¡Œæ—¶é—´: {elapsed_time/60:.1f} åˆ†é’Ÿ")
            print(f"{'â”€'*100}")

            # å®šæœŸä¿å­˜æ—¥å¿—
            if session_id % save_log_every == 0:
                save_log(results, api_base, model, test_start_time, current_time)
                print(f"ğŸ’¾ æ—¥å¿—å·²ä¿å­˜ (ç¬¬ {session_id} æ¬¡æµ‹è¯•)")

            # å»¶è¿Ÿä¸‹ä¸€ä¸ªè¯·æ±‚
            if delay_between_requests > 0:
                time.sleep(delay_between_requests)

    except KeyboardInterrupt:
        print("\n\n" + "=" * 100)
        print("â¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        print("=" * 100)

        # ä¿å­˜æœ€ç»ˆæ—¥å¿—
        final_time = time.time()
        save_log(results, api_base, model, test_start_time, final_time)

        # æœ€ç»ˆç»Ÿè®¡
        print_final_statistics(results, test_start_time, final_time)


def save_log(
    results: List[dict],
    api_base: str,
    model: str,
    start_time: float,
    end_time: float
):
    """ä¿å­˜æµ‹è¯•æ—¥å¿—"""
    output_file = f"vllm_stability_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

    success_results = [r for r in results if r["success"]]
    success_count = len(success_results)
    total_count = len(results)

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"vLLM ç¨³å®šæ€§æµ‹è¯•æŠ¥å‘Š\n")
        f.write(f"{'=' * 100}\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {datetime.fromtimestamp(start_time).isoformat()} - {datetime.fromtimestamp(end_time).isoformat()}\n")
        f.write(f"API Base: {api_base}\n")
        f.write(f"Model: {model}\n")
        f.write(f"æ€»è¯·æ±‚æ•°: {total_count}\n")
        f.write(f"æˆåŠŸ: {success_count} ({success_count/total_count*100:.1f}%)\n")
        f.write(f"å¤±è´¥: {total_count - success_count} ({(total_count - success_count)/total_count*100:.1f}%)\n")

        if success_count > 0:
            total_time = sum(r["time_taken"] for r in success_results)
            total_tokens = sum(r["estimated_tokens"] for r in success_results)
            avg_response_time = total_time / success_count
            avg_tokens = total_tokens / success_count
            avg_token_speed = total_tokens / total_time if total_time > 0 else 0

            f.write(f"\nå¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s\n")
            f.write(f"å¹³å‡ tokens: {avg_tokens:.0f} tokens/è¯·æ±‚\n")
            f.write(f"å¹³å‡é€Ÿåº¦: {avg_token_speed:.1f} tokens/s\n")
            f.write(f"æœ€å¿«å“åº”: {min(r['time_taken'] for r in success_results):.2f}s\n")
            f.write(f"æœ€æ…¢å“åº”: {max(r['time_taken'] for r in success_results):.2f}s\n")
            f.write(f"æœ€å¿«é€Ÿåº¦: {max(r['tokens_per_second'] for r in success_results):.1f} tokens/s\n")
            f.write(f"æœ€æ…¢é€Ÿåº¦: {min(r['tokens_per_second'] for r in success_results):.1f} tokens/s\n")

        f.write(f"\n{'=' * 100}\n\n")
        f.write(f"è¯¦ç»†è®°å½•:\n\n")

        for result in results:
            f.write(f"{'â”€' * 100}\n")
            f.write(f"ä¼šè¯ {result['session_id']}:\n")
            f.write(f"  æ—¶é—´: {result['timestamp']}\n")
            f.write(f"  é—®é¢˜: {result['question']}\n")
            f.write(f"  æˆåŠŸ: {result['success']}\n")
            f.write(f"  è€—æ—¶: {result['time_taken']:.2f}s\n")
            if result['success']:
                f.write(f"  å“åº”é•¿åº¦: {result['response_length']} å­—ç¬¦\n")
                f.write(f"  ä¼°è®¡ tokens: {result['estimated_tokens']}\n")
                f.write(f"  Token é€Ÿåº¦: {result['tokens_per_second']:.1f} tokens/s\n")
            else:
                f.write(f"  é”™è¯¯: {result['error']}\n")
            f.write("\n")

    return output_file


def print_final_statistics(results: List[dict], start_time: float, end_time: float):
    """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
    success_results = [r for r in results if r["success"]]
    success_count = len(success_results)
    total_count = len(results)
    total_time = end_time - start_time

    print()
    print("=" * 100)
    print("ğŸ“ˆ æœ€ç»ˆæµ‹è¯•ç»“æœç»Ÿè®¡")
    print("=" * 100)
    print(f"æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}s ({total_time/60:.1f} åˆ†é’Ÿ)")
    print(f"æ€»è¯·æ±‚æ•°: {total_count}")
    print(f"æˆåŠŸ: {success_count} ({success_count/total_count*100:.1f}%)")
    print(f"å¤±è´¥: {total_count - success_count} ({(total_count - success_count)/total_count*100:.1f}%)")
    print()

    if success_count > 0:
        total_response_time = sum(r["time_taken"] for r in success_results)
        total_tokens = sum(r["estimated_tokens"] for r in success_results)
        total_chars = sum(r["response_length"] for r in success_results)

        avg_response_time = total_response_time / success_count
        avg_tokens = total_tokens / success_count
        avg_token_speed = total_tokens / total_response_time if total_response_time > 0 else 0

        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s")
        print(f"å¹³å‡ tokens: {avg_tokens:.0f} tokens/è¯·æ±‚")
        print(f"å¹³å‡é€Ÿåº¦: {avg_token_speed:.1f} tokens/s")
        print()
        print(f"æœ€å¿«å“åº”: {min(r['time_taken'] for r in success_results):.2f}s")
        print(f"æœ€æ…¢å“åº”: {max(r['time_taken'] for r in success_results):.2f}s")
        print(f"æœ€å¿«é€Ÿåº¦: {max(r['tokens_per_second'] for r in success_results):.1f} tokens/s")
        print(f"æœ€æ…¢é€Ÿåº¦: {min(r['tokens_per_second'] for r in success_results):.1f} tokens/s")
        print()
        print(f"æ€»ç”Ÿæˆ tokens: {total_tokens} (~{total_chars} å­—ç¬¦)")

    print("=" * 100)


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    API_BASE = "http://192.168.18.9:8080/v1"
    MODEL = "/models"
    DELAY = 1.0  # æ¯æ¬¡è¯·æ±‚é—´éš” 1 ç§’
    TIMEOUT = 120.0  # è¶…æ—¶ 120 ç§’
    SAVE_LOG_EVERY = 10  # æ¯ 10 æ¬¡ä¿å­˜ä¸€æ¬¡æ—¥å¿—

    # è¿è¡Œæ— é™å¾ªç¯æµ‹è¯•
    run_infinite_stability_test(
        api_base=API_BASE,
        model=MODEL,
        delay_between_requests=DELAY,
        timeout=TIMEOUT,
        save_log_every=SAVE_LOG_EVERY,
    )
